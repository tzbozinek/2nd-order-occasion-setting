{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from theano import scan\n",
    "import theano.tensor as T\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import os, sys, subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('../data/', \"2nd POS Modeling Training Data OS2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DV'] = ((data['DV'].values - 1) / 2) - 1\n",
    "\n",
    "observed_R = data.pivot(columns = 'ID', index = 'trialseq', values = 'DV').values[:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occasion Setting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In its default form, the code block below allows λbar, γ1, and γ2 to scale per their formulas. However, in order to improve α parameter estimation (as described in our report), please set each of these to 1. Please see comments in code below on which code to activate and deactivate by highlighting that code and pressing ctrl / (or, on Mac, command /). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_function(stimuli_shown, Λ, λ, prev_V, prev_Vbar, prev_P, prev_N, prev_P2, prev_N2, stimulus_type, OS1_type, OS2_type, αD, αDE, αDEF, αS, αE, αF, αEF, αG, αH, αM, αN, αMN, αU, αUM, αUMN):\n",
    "    \n",
    "    Λbar = T.zeros_like(Λ)\n",
    "    Λbar = T.inc_subtensor(Λbar[0,:], (prev_V[5,:] > 0) * (1 - Λ[0, :])) #Dcs\n",
    "    Λbar = T.inc_subtensor(Λbar[1,:], (prev_V[3,:] > 0) * (1 - Λ[3, :])) #D1\n",
    "    Λbar = T.inc_subtensor(Λbar[2,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #D2\n",
    "    Λbar = T.inc_subtensor(Λbar[3,:], (prev_V[3,:] > 0) * (1 - Λ[3, :])) #Ecs\n",
    "    Λbar = T.inc_subtensor(Λbar[4,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #E1\n",
    "    Λbar = T.inc_subtensor(Λbar[5,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #F\n",
    "    Λbar = T.inc_subtensor(Λbar[6,:], (prev_V[5,:] > 0) * (1 - Λ[6, :])) #S\n",
    "    Λbar = T.inc_subtensor(Λbar[7,:], (prev_V[7,:] > 0) * (1 - Λ[7, :])) #G\n",
    "    Λbar = T.inc_subtensor(Λbar[8,:], (prev_V[7,:] > 0) * (1 - Λ[7, :])) #H\n",
    "    Λbar = T.inc_subtensor(Λbar[9,:], (prev_V[9,:] > 0) * (1 - Λ[9, :])) #Mcs\n",
    "    Λbar = T.inc_subtensor(Λbar[10,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #M1\n",
    "    Λbar = T.inc_subtensor(Λbar[11,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #N\n",
    "    Λbar = T.inc_subtensor(Λbar[12,:], (prev_V[11,:] > 0) * (1 - Λ[12, :])) #Ucs\n",
    "    Λbar = T.inc_subtensor(Λbar[13,:], (prev_V[9,:] > 0) * (1 - Λ[9, :])) #U1\n",
    "    Λbar = T.inc_subtensor(Λbar[14,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #U2\n",
    "    Λbar = T.inc_subtensor(Λbar[15,:], (prev_V[3,:] > 0) * (1 - Λ[3, :])) #D1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[16,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #D2_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[17,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #E1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[18,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #M1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[19,:], (prev_V[9,:] > 0) * (1 - Λ[10, :])) #U1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[20,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #U2_abs\n",
    "    \n",
    "    #To make λbar = 1, activate \"λbar = T.ones_like(Λbar)\" code below and \n",
    "    #deactivate all \"λbar =\" code under \"Code for λbar Scaling\" below.\n",
    "    \n",
    "    #Code for λbar = 1\n",
    "#     λbar = T.ones_like(Λbar)\n",
    "    \n",
    "    #Code for λbar Scaling\n",
    "    λbar = T.zeros_like(Λbar)\n",
    "    λbar = T.inc_subtensor(λbar[0,:], prev_V[5,:]) #Dcs\n",
    "    λbar = T.inc_subtensor(λbar[1,:], prev_V[3,:]) #D1\n",
    "    λbar = T.inc_subtensor(λbar[2,:], prev_V[5,:]) #D2\n",
    "    λbar = T.inc_subtensor(λbar[3,:], prev_V[5,:]) #Ecs\n",
    "    λbar = T.inc_subtensor(λbar[4,:], prev_V[5,:]) #E1\n",
    "    λbar = T.inc_subtensor(λbar[5,:], prev_V[5,:]) #F\n",
    "    λbar = T.inc_subtensor(λbar[6,:], prev_V[5,:]) #S\n",
    "    λbar = T.inc_subtensor(λbar[7,:], prev_V[7,:]) #G\n",
    "    λbar = T.inc_subtensor(λbar[8,:], prev_V[7,:]) #H\n",
    "    λbar = T.inc_subtensor(λbar[9,:], prev_V[9,:]) #Mcs\n",
    "    λbar = T.inc_subtensor(λbar[10,:], prev_V[11,:]) #M1\n",
    "    λbar = T.inc_subtensor(λbar[11,:], prev_V[11,:]) #N\n",
    "    λbar = T.inc_subtensor(λbar[12,:], prev_V[11,:]) #Ucs\n",
    "    λbar = T.inc_subtensor(λbar[13,:], prev_V[9,:]) #U1\n",
    "    λbar = T.inc_subtensor(λbar[14,:], prev_V[11,:]) #U2\n",
    "    λbar = T.inc_subtensor(λbar[15,:], prev_V[3,:]) #D1_abs\n",
    "    λbar = T.inc_subtensor(λbar[16,:], prev_V[5,:]) #D2_abs\n",
    "    λbar = T.inc_subtensor(λbar[17,:], prev_V[5,:]) #E1_abs\n",
    "    λbar = T.inc_subtensor(λbar[18,:], prev_V[11,:]) #M1_abs\n",
    "    λbar = T.inc_subtensor(λbar[19,:], prev_V[9,:]) #U1_abs\n",
    "    λbar = T.inc_subtensor(λbar[20,:], prev_V[11,:]) #U2_abs\n",
    "    \n",
    "    \n",
    "    #Prediction error\n",
    "    pe_V = λ - prev_V\n",
    "    pe_Vbar = λbar - prev_Vbar\n",
    "    pe_P = λ - prev_P\n",
    "    pe_N = λbar - prev_N\n",
    "    pe_P2 = λ - prev_P2\n",
    "    pe_N2 = λbar - prev_N2\n",
    "    \n",
    "    #Stimulus-Specific α\n",
    "    αD = αD*(stimuli_shown[21,:] > 0)\n",
    "    αDE = αDE*(stimuli_shown[22,:] > 0)\n",
    "    αDEF = αDEF*(stimuli_shown[23,:] > 0)\n",
    "    αS = αS*(stimuli_shown[24,:] > 0)\n",
    "    αE = αE*(stimuli_shown[25,:] > 0)\n",
    "    αF = αF*(stimuli_shown[26,:] > 0)\n",
    "    αEF = αEF*(stimuli_shown[27,:] > 0)\n",
    "    αG = αG*(stimuli_shown[28,:] > 0)\n",
    "    αH = αH*(stimuli_shown[29,:] > 0)\n",
    "    αM = αM*(stimuli_shown[30,:] > 0)\n",
    "    αN = αN*(stimuli_shown[31,:] > 0)\n",
    "    αMN = αMN*(stimuli_shown[32,:] > 0)\n",
    "    αU = αU*(stimuli_shown[33,:] > 0)\n",
    "    αUM = αUM*(stimuli_shown[34,:] > 0)\n",
    "    αUMN = αUMN*(stimuli_shown[35,:] > 0)\n",
    "\n",
    "    #To make γ = 1, activate code under \"Code for γ = 1\" and \n",
    "    #deactivate code under \"Code for γ Scaling\" below.\n",
    "    \n",
    "    #Code for γ = 1\n",
    "#     prev_γ1 = T.zeros_like(prev_V)\n",
    "#     prev_γ1 = T.set_subtensor(prev_γ1[stimulus_type.nonzero(),:],1)\n",
    "#     prev_γ2 = T.zeros_like(prev_V)\n",
    "#     prev_γ2 = T.set_subtensor(prev_γ2[OS1_type.nonzero(),:],1)\n",
    "    \n",
    "    #Code for γ Scaling\n",
    "    prev_γ1 = prev_V * prev_Vbar\n",
    "    prev_γ2 = prev_P * prev_N\n",
    "\n",
    "    #Delta formulas\n",
    "    CS_prev_γ1 = (prev_γ1 * stimuli_shown).sum(axis=0)\n",
    "    CS_prev_γ2 = (prev_γ2 * stimuli_shown).sum(axis=0)\n",
    "    ΔV = Λ * (αD + αDE + αDEF + αS + αE + αF + αEF + αG + αH + αM + αN + αMN + αU + αUM + αUMN) * pe_V\n",
    "    ΔVbar = Λbar * (αD + αDE + αDEF + αS + αE + αF + αEF + αG + αH + αM + αN + αMN + αU + αUM + αUMN) * pe_Vbar\n",
    "    ΔP = Λ * (αD + αDE + αDEF + αS + αE + αF + αEF + αG + αH + αM + αN + αMN + αU + αUM + αUMN) * CS_prev_γ1 * pe_P\n",
    "    ΔN = Λbar * (αD + αDE + αDEF + αS + αE + αF + αEF + αG + αH + αM + αN + αMN + αU + αUM + αUMN) * CS_prev_γ1 * pe_N\n",
    "    ΔP2 = Λ * (αD + αDE + αDEF + αS + αE + αF + αEF + αG + αH + αM + αN + αMN + αU + αUM + αUMN) * CS_prev_γ2 * pe_P2\n",
    "    ΔN2 = Λbar * (αD + αDE + αDEF + αS + αE + αF + αEF + αG + αH + αM + αN + αMN + αU + αUM + αUMN) * CS_prev_γ2 * pe_N2\n",
    "\n",
    "\n",
    "    # Only update stimuli that were shown\n",
    "    ΔV = ΔV * stimuli_shown\n",
    "    ΔVbar = ΔVbar * stimuli_shown\n",
    "    ΔP = ΔP * stimuli_shown\n",
    "    ΔN = ΔN * stimuli_shown\n",
    "    ΔP2 = ΔP2 * stimuli_shown\n",
    "    ΔN2 = ΔN2 * stimuli_shown\n",
    "    \n",
    "    # Update V, Vbar, P, N, P2, N2\n",
    "    V = T.zeros_like(prev_V)\n",
    "    Vbar = T.zeros_like(prev_Vbar)\n",
    "    P = T.zeros_like(prev_P)\n",
    "    N = T.zeros_like(prev_N)\n",
    "    P2 = T.zeros_like(prev_P2)\n",
    "    N2 = T.zeros_like(prev_N2)\n",
    "    \n",
    "    # Only update V and Vbar for CSs. Only update P and N for 1st-order OSs. Only update P2 and N2 for 2nd-order OSs.\n",
    "    V = T.inc_subtensor(V[T.eq(stimulus_type, 1)], prev_V[T.eq(stimulus_type, 1)] + ΔV[T.eq(stimulus_type, 1)])\n",
    "    Vbar = T.inc_subtensor(Vbar[T.eq(stimulus_type, 1)], prev_Vbar[T.eq(stimulus_type, 1)] + ΔVbar[T.eq(stimulus_type, 1)])\n",
    "    P = T.inc_subtensor(P[T.eq(OS1_type, 1)], prev_P[T.eq(OS1_type, 1)] + ΔP[T.eq(OS1_type, 1)])\n",
    "    N = T.inc_subtensor(N[T.eq(OS1_type, 1)], prev_N[T.eq(OS1_type, 1)] + ΔN[T.eq(OS1_type, 1)])\n",
    "    P2 = T.inc_subtensor(P2[T.eq(OS2_type, 1)], prev_P2[T.eq(OS2_type, 1)] + ΔP2[T.eq(OS2_type, 1)])\n",
    "    N2 = T.inc_subtensor(N2[T.eq(OS2_type, 1)], prev_N2[T.eq(OS2_type, 1)] + ΔN2[T.eq(OS2_type, 1)])\n",
    "    \n",
    "    return V, Vbar, P, N, P2, N2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Simulated Data with Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In code below, use \"Code for randomized α values\" to simulate random α values, which will then be estimated by our model. This was used for our simulated vs recovered plots in the manuscript. In order to graph \"perfect\" learning rate data, use \"Code for α = 1,\" which was plotted in our manuscript alongside participants with low, medium, and high α parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stim = 36\n",
    "n_subjects = len(data['ID'].unique())\n",
    "\n",
    "#Initial values\n",
    "R = np.zeros((n_stim, n_subjects))\n",
    "overall_R = np.zeros((1, n_subjects))\n",
    "v_excitatory = np.zeros((n_stim, n_subjects))\n",
    "v_inhibitory = np.zeros((n_stim, n_subjects))\n",
    "P = np.zeros((n_stim, n_subjects))\n",
    "N = np.zeros((n_stim, n_subjects))\n",
    "P2 = np.zeros((n_stim, n_subjects))\n",
    "N2 = np.zeros((n_stim, n_subjects))\n",
    "\n",
    "#Code for randomized α values\n",
    "gen_dist = pm.Beta.dist(2, 2, shape = n_subjects)\n",
    "αD_subject_sim = gen_dist.random()\n",
    "αDE_subject_sim = gen_dist.random()\n",
    "αDEF_subject_sim = gen_dist.random()\n",
    "αS_subject_sim = gen_dist.random()\n",
    "αE_subject_sim = gen_dist.random()\n",
    "αF_subject_sim = gen_dist.random()\n",
    "αEF_subject_sim = gen_dist.random()\n",
    "αG_subject_sim = gen_dist.random()\n",
    "αH_subject_sim = gen_dist.random()\n",
    "αM_subject_sim = gen_dist.random()\n",
    "αN_subject_sim = gen_dist.random()\n",
    "αMN_subject_sim = gen_dist.random()\n",
    "αU_subject_sim = gen_dist.random()\n",
    "αUM_subject_sim = gen_dist.random()\n",
    "αUMN_subject_sim = gen_dist.random()\n",
    "\n",
    "\n",
    "#Code for α = 1\n",
    "# gen_dist = pm.Beta.dist(2, 2, shape=n_subjects)\n",
    "# αD_subject_sim = np.ones(n_subjects)\n",
    "# αDE_subject_sim = np.ones(n_subjects)\n",
    "# αDEF_subject_sim = np.ones(n_subjects)\n",
    "# αS_subject_sim = np.ones(n_subjects)\n",
    "# αE_subject_sim = np.ones(n_subjects)\n",
    "# αF_subject_sim = np.ones(n_subjects)\n",
    "# αEF_subject_sim = np.ones(n_subjects)\n",
    "# αG_subject_sim = np.ones(n_subjects)\n",
    "# αH_subject_sim = np.ones(n_subjects)\n",
    "# αM_subject_sim = np.ones(n_subjects)\n",
    "# αN_subject_sim = np.ones(n_subjects)\n",
    "# αMN_subject_sim = np.ones(n_subjects)\n",
    "# αU_subject_sim = np.ones(n_subjects)\n",
    "# αUM_subject_sim = np.ones(n_subjects)\n",
    "# αUMN_subject_sim = np.ones(n_subjects)\n",
    "\n",
    "#US values\n",
    "small_lambda = data.pivot(index='trialseq', values='US', columns='ID').values[:, np.newaxis, :].repeat(36, axis=1).astype(float)\n",
    "stim_data = []\n",
    "\n",
    "for sub in data['ID'].unique():\n",
    "    stim_data.append(data.loc[data['ID'] == sub, ['Dcs', 'D1', 'D2', 'Ecs', 'E1', 'F', 'S', 'G', 'H', 'Mcs', \n",
    "                                                  'M1', 'N', 'Ucs', 'U1', 'U2', 'D1_abs', 'D2_abs', 'E1_abs', \n",
    "                                                  'M1_abs', 'U1_abs', 'U2_abs',\n",
    "                                                  'alpha_D', 'alpha_DE', 'alpha_DEF', 'alpha_S', 'alpha_E', 'alpha_F',\n",
    "                                                  'alpha_EF', 'alpha_G', 'alpha_H', 'alpha_M', 'alpha_N', 'alpha_MN',\n",
    "                                                  'alpha_U', 'alpha_UM', 'alpha_UMN']].values)\n",
    "\n",
    "stimuli_shown = np.dstack(stim_data)\n",
    "big_lambda = small_lambda\n",
    "\n",
    "#Add imaginary -1th trial\n",
    "big_lambda = np.vstack([np.zeros((1, n_stim, n_subjects)), big_lambda[:-1, ...]]).astype(float) # Add one trial of zeros to the start, remove the last trial\n",
    "small_lambda = big_lambda\n",
    "stimuli_shown = np.vstack([np.zeros((1, n_stim, n_subjects)), stimuli_shown]) # Add one trial of zeros to the start, DO NOT remove the last trial - this is needed for prediction\n",
    "\n",
    "#Designate stimulus types\n",
    "stimulus_type = np.ones(n_stim)\n",
    "stimulus_type[[1, 4, 10, 13, 15, 17, 18, 19, 2, 14, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]] = 0 #make all OSs = 0\n",
    "\n",
    "OS1_type = np.zeros(n_stim)\n",
    "OS1_type[[1, 4, 10, 13, 15, 17, 18, 19]] = 1 #make 1st OSs = 1\n",
    "\n",
    "OS2_type = np.zeros(n_stim)\n",
    "OS2_type[[2, 14, 16, 20]] = 1 #make 2nd OSs = 1\n",
    "\n",
    "\n",
    "#Convert task outcomes to tensors\n",
    "big_lambda = T.as_tensor_variable(big_lambda.astype(float))\n",
    "small_lambda = T.as_tensor_variable(small_lambda.astype(float))\n",
    "stimuli_shown = T.as_tensor_variable(stimuli_shown)\n",
    "\n",
    "stimuli_shown_sim = stimuli_shown.copy()\n",
    "big_lambda_sim = big_lambda.copy()\n",
    "small_lambda_sim = small_lambda.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Fake Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the loop\n",
    "output, updates = scan(fn=learning_function,\n",
    "                    sequences=[{'input': stimuli_shown_sim[:-1, ...]},\n",
    "                             {'input': big_lambda_sim},\n",
    "                             {'input': small_lambda_sim}],\n",
    "                    outputs_info=[v_excitatory, v_inhibitory, P, N, P2, N2],\n",
    "                    non_sequences = [stimulus_type, OS1_type, OS2_type, αD_subject_sim, αDE_subject_sim, αDEF_subject_sim, αS_subject_sim, αE_subject_sim, αF_subject_sim, αEF_subject_sim, αG_subject_sim, αH_subject_sim, αM_subject_sim, αN_subject_sim, αMN_subject_sim, αU_subject_sim, αUM_subject_sim, αUMN_subject_sim])\n",
    "\n",
    "#Get model output\n",
    "V_out, Vbar_out, P_out, N_out, P2_out, N2_out = [i.eval() for i in output]\n",
    "\n",
    "estimated_overall_R = ((V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) - (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) + \\\n",
    "    ((P_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) - \\\n",
    "    ((N_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) + \\\n",
    "    ((P2_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (N_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (P_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (N_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) - \\\n",
    "    ((N2_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (P_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (P_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (N_out * stimuli_shown_sim[1:, ...]).sum(axis=1))\n",
    "\n",
    "overall_R_sim = estimated_overall_R.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = len(data['ID'].unique())\n",
    "\n",
    "#Initial values\n",
    "R = np.zeros((n_stim, n_subjects))\n",
    "\n",
    "#US values\n",
    "small_lambda = data.pivot(index='trialseq', values='US', columns='ID').values[:, np.newaxis, :].repeat(36, axis=1).astype(float)\n",
    "stim_data = []\n",
    "\n",
    "for sub in data['ID'].unique():\n",
    "    stim_data.append(data.loc[data['ID'] == sub, ['Dcs', 'D1', 'D2', 'Ecs', 'E1', 'F', 'S', 'G', 'H', 'Mcs', \n",
    "                                                  'M1', 'N', 'Ucs', 'U1', 'U2', 'D1_abs', 'D2_abs', 'E1_abs', \n",
    "                                                  'M1_abs', 'U1_abs', 'U2_abs',\n",
    "                                                  'alpha_D', 'alpha_DE', 'alpha_DEF', 'alpha_S', 'alpha_E', 'alpha_F',\n",
    "                                                  'alpha_EF', 'alpha_G', 'alpha_H', 'alpha_M', 'alpha_N', 'alpha_MN',\n",
    "                                                  'alpha_U', 'alpha_UM', 'alpha_UMN']].values)\n",
    "\n",
    "stimuli_shown = np.dstack(stim_data)\n",
    "big_lambda = small_lambda\n",
    "\n",
    "#Add imaginary -1th trial\n",
    "big_lambda = np.vstack([np.zeros((1, n_stim, n_subjects)), big_lambda[:-1, ...]]).astype(float) # Add one trial of zeros to the start, remove the last trial\n",
    "small_lambda = big_lambda\n",
    "stimuli_shown = np.vstack([np.zeros((1, n_stim, n_subjects)), stimuli_shown]) # Add one trial of zeros to the start, DO NOT remove the last trial - this is needed for prediction\n",
    "\n",
    "stimulus_type = np.ones(n_stim)\n",
    "stimulus_type[[1, 4, 10, 13, 15, 17, 18, 19, 2, 14, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]] = 0 #make all OSs = 0\n",
    "\n",
    "OS1_type = np.zeros(n_stim)\n",
    "OS1_type[[1, 4, 10, 13, 15, 17, 18, 19]] = 1 #make 1st OSs = 1\n",
    "\n",
    "OS2_type = np.zeros(n_stim)\n",
    "OS2_type[[2, 14, 16, 20]] = 1 #make 2nd OSs = 1\n",
    "\n",
    "#Convert task outcomes to tensors\n",
    "big_lambda = T.as_tensor_variable(big_lambda.astype(float))\n",
    "small_lambda = T.as_tensor_variable(small_lambda.astype(float))\n",
    "stimuli_shown = T.as_tensor_variable(stimuli_shown)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    \n",
    "    # Learning rate lies between 0 and 1, so we use a beta distribution\n",
    "    αD_mean = pm.Normal('αD_mean', 0.5, 10)\n",
    "    αD_sd = pm.HalfCauchy('αD_sd', 10)\n",
    "    \n",
    "    αDE_mean = pm.Normal('αDE_mean', 0.5, 10)\n",
    "    αDE_sd = pm.HalfCauchy('αDE_sd', 10)\n",
    "    \n",
    "    αDEF_mean = pm.Normal('αDEF_mean', 0.5, 10)\n",
    "    αDEF_sd = pm.HalfCauchy('αDEF_sd', 10)\n",
    "    \n",
    "    αS_mean = pm.Normal('αS_mean', 0.5, 10)\n",
    "    αS_sd = pm.HalfCauchy('αS_sd', 10)\n",
    "    \n",
    "    αE_mean = pm.Normal('αE_mean', 0.5, 10)\n",
    "    αE_sd = pm.HalfCauchy('αE_sd', 10)\n",
    "    \n",
    "    αF_mean = pm.Normal('αF_mean', 0.5, 10)\n",
    "    αF_sd = pm.HalfCauchy('αF_sd', 10)\n",
    "    \n",
    "    αEF_mean = pm.Normal('αEF_mean', 0.5, 10)\n",
    "    αEF_sd = pm.HalfCauchy('αEF_sd', 10)\n",
    "    \n",
    "    αG_mean = pm.Normal('αG_mean', 0.5, 10)\n",
    "    αG_sd = pm.HalfCauchy('αG_sd', 10)\n",
    "    \n",
    "    αH_mean = pm.Normal('αH_mean', 0.5, 10)\n",
    "    αH_sd = pm.HalfCauchy('αH_sd', 10)\n",
    "    \n",
    "    αM_mean = pm.Normal('αM_mean', 0.5, 10)\n",
    "    αM_sd = pm.HalfCauchy('αM_sd', 10)\n",
    "    \n",
    "    αN_mean = pm.Normal('αN_mean', 0.5, 10)\n",
    "    αN_sd = pm.HalfCauchy('αN_sd', 10)\n",
    "    \n",
    "    αMN_mean = pm.Normal('αMN_mean', 0.5, 10)\n",
    "    αMN_sd = pm.HalfCauchy('αMN_sd', 10)\n",
    "    \n",
    "    αU_mean = pm.Normal('αU_mean', 0.5, 10)\n",
    "    αU_sd = pm.HalfCauchy('αU_sd', 10)\n",
    "    \n",
    "    αUM_mean = pm.Normal('αUM_mean', 0.5, 10)\n",
    "    αUM_sd = pm.HalfCauchy('αUM_sd', 10)\n",
    "    \n",
    "    αUMN_mean = pm.Normal('αUMN_mean', 0.5, 10)\n",
    "    αUMN_sd = pm.HalfCauchy('αUMN_sd', 10)\n",
    "\n",
    "    \n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=0, upper=1)\n",
    "    αD_subject = BoundedNormal('αD', mu=αD_mean, sd=αD_sd, shape=(n_subjects,))\n",
    "    αDE_subject = BoundedNormal('αDE', mu=αDE_mean, sd=αDE_sd, shape=(n_subjects,))\n",
    "    αDEF_subject = BoundedNormal('αDEF', mu=αDEF_mean, sd=αDEF_sd, shape=(n_subjects,))\n",
    "    αS_subject = BoundedNormal('αS', mu=αS_mean, sd=αS_sd, shape=(n_subjects,))\n",
    "    αE_subject = BoundedNormal('αE', mu=αE_mean, sd=αE_sd, shape=(n_subjects,))\n",
    "    αF_subject = BoundedNormal('αF', mu=αF_mean, sd=αF_sd, shape=(n_subjects,))\n",
    "    αEF_subject = BoundedNormal('αEF', mu=αEF_mean, sd=αEF_sd, shape=(n_subjects,))\n",
    "    αG_subject = BoundedNormal('αG', mu=αG_mean, sd=αG_sd, shape=(n_subjects,))\n",
    "    αH_subject = BoundedNormal('αH', mu=αH_mean, sd=αH_sd, shape=(n_subjects,))\n",
    "    αM_subject = BoundedNormal('αM', mu=αM_mean, sd=αM_sd, shape=(n_subjects,))\n",
    "    αN_subject = BoundedNormal('αN', mu=αN_mean, sd=αN_sd, shape=(n_subjects,))\n",
    "    αMN_subject = BoundedNormal('αMN', mu=αMN_mean, sd=αMN_sd, shape=(n_subjects,))\n",
    "    αU_subject = BoundedNormal('αU', mu=αU_mean, sd=αU_sd, shape=(n_subjects,))\n",
    "    αUM_subject = BoundedNormal('αUM', mu=αUM_mean, sd=αUM_sd, shape=(n_subjects,))\n",
    "    αUMN_subject = BoundedNormal('αUMN', mu=αUMN_mean, sd=αUMN_sd, shape=(n_subjects,))\n",
    "\n",
    "    \n",
    "    # Run the loop\n",
    "    output, updates = scan(fn=learning_function,\n",
    "                      sequences=[dict(input=stimuli_shown[:-1, ...]), dict(input=big_lambda), dict(input=small_lambda)],\n",
    "                      outputs_info=[v_excitatory, v_inhibitory, P, N, P2, N2],\n",
    "                      non_sequences = [stimulus_type, OS1_type, OS2_type, αD_subject, αDE_subject, αDEF_subject, αS_subject, αE_subject, αF_subject, αEF_subject, αG_subject, αH_subject, αM_subject, αN_subject, αMN_subject, αU_subject, αUM_subject, αUMN_subject])\n",
    "    \n",
    "    # Get model output\n",
    "    V, Vbar, P, N, P2, N2 = output\n",
    "    \n",
    "    # Calculate response - combine direct associations, 1st-order occasion setting, and 2nd-order occasion setting.\n",
    "    # As a reminder: γ1 = V*Vbar, and γ2 = P*N\n",
    "    R = (V - Vbar) + ((P * Vbar * V*Vbar) - (N * V *V*Vbar)) + (P2 * N * V * V*Vbar * P*N) - (N2 * P * Vbar * V*Vbar * P*N)\n",
    "\n",
    "    # # Single R value\n",
    "    estimated_overall_R = ((V * stimuli_shown[1:, ...]).sum(axis=1) - (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P2 * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N2 * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1))\n",
    "    \n",
    "    # This allows us to output the estimated R\n",
    "    estimated_overall_R = pm.Deterministic('estimated_overall_R', estimated_overall_R)\n",
    "    \n",
    "    # Reshape output of the model and get categorical likelihood\n",
    "    sigma = pm.HalfCauchy('sigma', 0.5)\n",
    "    likelihood = pm.Normal('likelihood', mu=estimated_overall_R, sigma=sigma, observed=pd.DataFrame(overall_R_sim.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Data (go to \"Real Data\" below if you want to skip data simulations and just get real data results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.variational.callbacks import CheckParametersConvergence\n",
    "with model:\n",
    "    approx = pm.fit(method='advi', n=40000, callbacks=[CheckParametersConvergence()])\n",
    "trace = approx.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_output = pm.summary(trace, kind='stats', varnames=[i for i in model.named_vars if 'α' in i and not i in model.deterministics and not 'log' in i and not 'interval' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, var_names=['αD_mean', 'αDE_mean', 'αDEF_mean', 'αS_mean', 'αE_mean', 'αF_mean', 'αEF_mean', 'αG_mean', 'αH_mean', 'αM_mean', 'αN_mean', 'αMN_mean', 'αU_mean', 'αUM_mean', 'αUMN_mean']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_data_var = {'Simulated_αD': αD_subject_sim, 'Recovered_αD': trace['αD'].mean(axis=0), \n",
    "                      'Simulated_αDE': αDE_subject_sim, 'Recovered_αDE': trace['αDE'].mean(axis=0),\n",
    "                      'Simulated_αDEF': αDEF_subject_sim, 'Recovered_αDEF': trace['αDEF'].mean(axis=0),\n",
    "                      'Simulated_αS': αS_subject_sim, 'Recovered_αS': trace['αS'].mean(axis=0),\n",
    "                      'Simulated_αE': αE_subject_sim, 'Recovered_αE': trace['αE'].mean(axis=0),\n",
    "                      'Simulated_αF': αF_subject_sim, 'Recovered_αF': trace['αF'].mean(axis=0),\n",
    "                      'Simulated_αEF': αEF_subject_sim, 'Recovered_αEF': trace['αEF'].mean(axis=0),\n",
    "                      'Simulated_αG': αG_subject_sim, 'Recovered_αG': trace['αG'].mean(axis=0),\n",
    "                      'Simulated_αH': αH_subject_sim, 'Recovered_αH': trace['αH'].mean(axis=0),\n",
    "                      'Simulated_αM': αM_subject_sim, 'Recovered_αM': trace['αM'].mean(axis=0),\n",
    "                      'Simulated_αN': αN_subject_sim, 'Recovered_αN': trace['αN'].mean(axis=0),\n",
    "                      'Simulated_αMN': αMN_subject_sim, 'Recovered_αMN': trace['αMN'].mean(axis=0),\n",
    "                      'Simulated_αU': αU_subject_sim, 'Recovered_αU': trace['αU'].mean(axis=0),\n",
    "                      'Simulated_αUMN': αUM_subject_sim, 'Recovered_αUM': trace['αUM'].mean(axis=0),\n",
    "                      'Simulated_αUMN': αUMN_subject_sim, 'Recovered_αUMN': trace['αUMN'].mean(axis=0)}\n",
    "\n",
    "recovered_data_var = pd.DataFrame(recovered_data_var)\n",
    "recovered_data_var.to_csv(os.path.join('../output/',r'2nd POS - OS2 Stimulus-Specific, Simulated vs Recovered.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(3, 5, sharex = True, sharey = True, figsize=(12, 7.5))\n",
    "f.suptitle('Simulated vs Recovered α Parameters', y=1.02, fontsize = 16)\n",
    "f.text(.5, -.02, 'Simulated α', va='center', ha='center', fontsize = 16)\n",
    "f.text(-.02, .5, 'Recovered α', va='center', ha='center', fontsize = 16, rotation=90)\n",
    "\n",
    "sns.regplot(αD_subject_sim, trace['αD'].mean(axis=0), label='αD_subject', ax=ax[0,0], color = 'black')\n",
    "sns.regplot(αDE_subject_sim, trace['αDE'].mean(axis=0), label='αDE_subject', ax=ax[0,1], color = 'black')\n",
    "sns.regplot(αDEF_subject_sim, trace['αDEF'].mean(axis=0), label='αDEF_subject', ax=ax[0,2], color = 'black')\n",
    "sns.regplot(αS_subject_sim, trace['αS'].mean(axis=0), label='αS_subject', ax=ax[0,3], color = 'black')\n",
    "sns.regplot(αE_subject_sim, trace['αE'].mean(axis=0), label='αE_subject', ax=ax[0,4], color = 'black')\n",
    "sns.regplot(αF_subject_sim, trace['αF'].mean(axis=0), label='αF_subject', ax=ax[1,0], color = 'black')\n",
    "sns.regplot(αEF_subject_sim, trace['αEF'].mean(axis=0), label='αEF_subject', ax=ax[1,1], color = 'black')\n",
    "sns.regplot(αG_subject_sim, trace['αG'].mean(axis=0), label='αG_subject', ax=ax[1,2], color = 'black')\n",
    "sns.regplot(αH_subject_sim, trace['αH'].mean(axis=0), label='αH_subject', ax=ax[1,3], color = 'black')\n",
    "sns.regplot(αM_subject_sim, trace['αM'].mean(axis=0), label='αM_subject', ax=ax[1,4], color = 'black')\n",
    "sns.regplot(αN_subject_sim, trace['αN'].mean(axis=0), label='αN_subject', ax=ax[2,0], color = 'black')\n",
    "sns.regplot(αMN_subject_sim, trace['αMN'].mean(axis=0), label='αMN_subject', ax=ax[2,1], color = 'black')\n",
    "sns.regplot(αU_subject_sim, trace['αU'].mean(axis=0), label='αU_subject', ax=ax[2,2], color = 'black')\n",
    "sns.regplot(αUM_subject_sim, trace['αUM'].mean(axis=0), label='αUM_subject', ax=ax[2,3], color = 'black')\n",
    "sns.regplot(αUMN_subject_sim, trace['αUMN'].mean(axis=0), label='αUMN_subject', ax=ax[2,4], color = 'black')\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        ax[0,0].set_title('α D')\n",
    "        ax[0,1].set_title('α DE')\n",
    "        ax[0,2].set_title('α DEF')\n",
    "        ax[0,3].set_title('α S')\n",
    "        ax[0,4].set_title('α E')\n",
    "        ax[1,0].set_title('α F')\n",
    "        ax[1,1].set_title('α EF')\n",
    "        ax[1,2].set_title('α G')\n",
    "        ax[1,3].set_title('α H')\n",
    "        ax[1,4].set_title('α M')\n",
    "        ax[2,0].set_title('α N')\n",
    "        ax[2,1].set_title('α MN')\n",
    "        ax[2,2].set_title('α U')\n",
    "        ax[2,3].set_title('α UM')\n",
    "        ax[2,4].set_title('α UMN')\n",
    "\n",
    "plt.setp(ax, xticks=[0, .2, .4, .6, .8, 1], yticks=[0, .2, .4, .6, .8, 1])        \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join('../output/',r'2nd POS - OS2 Stimulus-Specific, Simulated vs Recovered.svg'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model to Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = len(data['ID'].unique())\n",
    "n_stim = 36\n",
    "\n",
    "# Initial values\n",
    "R = np.zeros((n_stim, n_subjects))  # Value estimate\n",
    "overall_R = np.zeros((1, n_subjects))\n",
    "v_excitatory = np.zeros((n_stim, n_subjects)) \n",
    "v_inhibitory = np.zeros((n_stim, n_subjects)) \n",
    "P = np.zeros((n_stim, n_subjects))\n",
    "N = np.zeros((n_stim, n_subjects))\n",
    "P2 = np.zeros((n_stim, n_subjects))\n",
    "N2 = np.zeros((n_stim, n_subjects))\n",
    "gamma = np.ones((n_stim, n_subjects))\n",
    "\n",
    "# US values\n",
    "small_lambda = data.pivot(index='trialseq', values='US', columns='ID').values[:, np.newaxis, :].repeat(n_stim, axis=1)\n",
    "stim_data = []\n",
    "\n",
    "for sub in data['ID'].unique():\n",
    "    stim_data.append(data.loc[data['ID'] == sub, ['Dcs', 'D1', 'D2', 'Ecs', 'E1', 'F', 'S', 'G', 'H', 'Mcs', \n",
    "                                                  'M1', 'N', 'Ucs', 'U1', 'U2', 'D1_abs', 'D2_abs', 'E1_abs', \n",
    "                                                  'M1_abs', 'U1_abs', 'U2_abs',\n",
    "                                                  'alpha_D', 'alpha_DE', 'alpha_DEF', 'alpha_S', 'alpha_E', 'alpha_F',\n",
    "                                                  'alpha_EF', 'alpha_G', 'alpha_H', 'alpha_M', 'alpha_N', 'alpha_MN',\n",
    "                                                  'alpha_U', 'alpha_UM', 'alpha_UMN']].values)\n",
    "    \n",
    "stimuli_shown = np.dstack(stim_data)\n",
    "big_lambda = small_lambda\n",
    "\n",
    "# Add imaginary -1th trial\n",
    "big_lambda = np.vstack([np.zeros((1, n_stim, n_subjects)), big_lambda[:-1, ...]])  # Add one trial of zeros to the start, remove the last trial\n",
    "small_lambda = big_lambda\n",
    "stimuli_shown = np.vstack([np.zeros((1, n_stim, n_subjects)), stimuli_shown]) # Add one trial of zeros to the start, DO NOT remove the last trial - this is needed for prediction\n",
    "\n",
    "stimulus_type = np.ones(n_stim)\n",
    "stimulus_type[[1, 4, 10, 13, 15, 17, 18, 19, 2, 14, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]] = 0 #make all OSs = 0\n",
    "\n",
    "OS1_type = np.zeros(n_stim)\n",
    "OS1_type[[1, 4, 10, 13, 15, 17, 18, 19]] = 1 #make 1st OSs = 1\n",
    "\n",
    "OS2_type = np.zeros(n_stim)\n",
    "OS2_type[[2, 14, 16, 20]] = 1 #make 2nd OSs = 1\n",
    "\n",
    "# Convert task outcomes to tensors\n",
    "big_lambda = T.as_tensor_variable(big_lambda)\n",
    "small_lambda = T.as_tensor_variable(small_lambda)\n",
    "stimuli_shown = T.as_tensor_variable(stimuli_shown)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    \n",
    "    # Learning rate lies between 0 and 1 so we use a beta distribution\n",
    "    αD_mean = pm.Normal('αD_mean', 0.5, 10)\n",
    "    αD_sd = pm.HalfCauchy('αD_sd', 10)\n",
    "    \n",
    "    αDE_mean = pm.Normal('αDE_mean', 0.5, 10)\n",
    "    αDE_sd = pm.HalfCauchy('αDE_sd', 10)\n",
    "    \n",
    "    αDEF_mean = pm.Normal('αDEF_mean', 0.5, 10)\n",
    "    αDEF_sd = pm.HalfCauchy('αDEF_sd', 10)\n",
    "    \n",
    "    αS_mean = pm.Normal('αS_mean', 0.5, 10)\n",
    "    αS_sd = pm.HalfCauchy('αS_sd', 10)\n",
    "    \n",
    "    αE_mean = pm.Normal('αE_mean', 0.5, 10)\n",
    "    αE_sd = pm.HalfCauchy('αE_sd', 10)\n",
    "    \n",
    "    αF_mean = pm.Normal('αF_mean', 0.5, 10)\n",
    "    αF_sd = pm.HalfCauchy('αF_sd', 10)\n",
    "    \n",
    "    αEF_mean = pm.Normal('αEF_mean', 0.5, 10)\n",
    "    αEF_sd = pm.HalfCauchy('αEF_sd', 10)\n",
    "    \n",
    "    αG_mean = pm.Normal('αG_mean', 0.5, 10)\n",
    "    αG_sd = pm.HalfCauchy('αG_sd', 10)\n",
    "    \n",
    "    αH_mean = pm.Normal('αH_mean', 0.5, 10)\n",
    "    αH_sd = pm.HalfCauchy('αH_sd', 10)\n",
    "    \n",
    "    αM_mean = pm.Normal('αM_mean', 0.5, 10)\n",
    "    αM_sd = pm.HalfCauchy('αM_sd', 10)\n",
    "    \n",
    "    αN_mean = pm.Normal('αN_mean', 0.5, 10)\n",
    "    αN_sd = pm.HalfCauchy('αN_sd', 10)\n",
    "    \n",
    "    αMN_mean = pm.Normal('αMN_mean', 0.5, 10)\n",
    "    αMN_sd = pm.HalfCauchy('αMN_sd', 10)\n",
    "    \n",
    "    αU_mean = pm.Normal('αU_mean', 0.5, 10)\n",
    "    αU_sd = pm.HalfCauchy('αU_sd', 10)\n",
    "    \n",
    "    αUM_mean = pm.Normal('αUM_mean', 0.5, 10)\n",
    "    αUM_sd = pm.HalfCauchy('αUM_sd', 10)\n",
    "    \n",
    "    αUMN_mean = pm.Normal('αUMN_mean', 0.5, 10)\n",
    "    αUMN_sd = pm.HalfCauchy('αUMN_sd', 10)\n",
    "\n",
    "    \n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=0, upper=1)\n",
    "    αD_subject = BoundedNormal('αD', mu=αD_mean, sd=αD_sd, shape=(n_subjects,))\n",
    "    αDE_subject = BoundedNormal('αDE', mu=αDE_mean, sd=αDE_sd, shape=(n_subjects,))\n",
    "    αDEF_subject = BoundedNormal('αDEF', mu=αDEF_mean, sd=αDEF_sd, shape=(n_subjects,))\n",
    "    αS_subject = BoundedNormal('αS', mu=αS_mean, sd=αS_sd, shape=(n_subjects,))\n",
    "    αE_subject = BoundedNormal('αE', mu=αE_mean, sd=αE_sd, shape=(n_subjects,))\n",
    "    αF_subject = BoundedNormal('αF', mu=αF_mean, sd=αF_sd, shape=(n_subjects,))\n",
    "    αEF_subject = BoundedNormal('αEF', mu=αEF_mean, sd=αEF_sd, shape=(n_subjects,))\n",
    "    αG_subject = BoundedNormal('αG', mu=αG_mean, sd=αG_sd, shape=(n_subjects,))\n",
    "    αH_subject = BoundedNormal('αH', mu=αH_mean, sd=αH_sd, shape=(n_subjects,))\n",
    "    αM_subject = BoundedNormal('αM', mu=αM_mean, sd=αM_sd, shape=(n_subjects,))\n",
    "    αN_subject = BoundedNormal('αN', mu=αN_mean, sd=αN_sd, shape=(n_subjects,))\n",
    "    αMN_subject = BoundedNormal('αMN', mu=αMN_mean, sd=αMN_sd, shape=(n_subjects,))\n",
    "    αU_subject = BoundedNormal('αU', mu=αU_mean, sd=αU_sd, shape=(n_subjects,))\n",
    "    αUM_subject = BoundedNormal('αUM', mu=αUM_mean, sd=αUM_sd, shape=(n_subjects,))\n",
    "    αUMN_subject = BoundedNormal('αUMN', mu=αUMN_mean, sd=αUMN_sd, shape=(n_subjects,))\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # Run the loop\n",
    "    output, updates = scan(fn=learning_function,\n",
    "                      sequences=[dict(input=stimuli_shown[:-1, ...]), dict(input=big_lambda), dict(input=small_lambda)],\n",
    "                      outputs_info=[v_excitatory, v_inhibitory, P, N, P2, N2],\n",
    "                      non_sequences = [stimulus_type, OS1_type, OS2_type, αD_subject, αDE_subject, αDEF_subject, αS_subject, αE_subject, αF_subject, αEF_subject, αG_subject, αH_subject, αM_subject, αN_subject, αMN_subject, αU_subject, αUM_subject, αUMN_subject])\n",
    "    \n",
    "    # Get model output\n",
    "    V, Vbar, P, N, P2, N2 = output\n",
    "    \n",
    "    # Calculate response - combine direct associations, 1st-order occasion setting, and 2nd-order occasion setting.\n",
    "    # As a reminder: γ1 = V*Vbar, and γ2 = P*N\n",
    "    R = (V - Vbar) + ((P * Vbar * V*Vbar) - (N * V *V*Vbar)) + (P2 * N * V * V*Vbar * P*N) - (N2 * P * Vbar * V*Vbar * P*N)\n",
    "\n",
    "    # # Single R value\n",
    "    estimated_overall_R = ((V * stimuli_shown[1:, ...]).sum(axis=1) - (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P2 * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N2 * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1))\n",
    "       \n",
    "    # This allows us to output the estimated R\n",
    "    estimated_overall_R = pm.Deterministic('estimated_overall_R', estimated_overall_R)\n",
    "    V = pm.Deterministic('estimated_V', V)\n",
    "    Vbar = pm.Deterministic('estimated_Vbar', Vbar)\n",
    "    P = pm.Deterministic('estimated_P', P)\n",
    "    N = pm.Deterministic('estimated_N', N)\n",
    "    P2 = pm.Deterministic('estimated_P2', P2)\n",
    "    N2 = pm.Deterministic('estimated_N2', N2)\n",
    "    γ1 = pm.Deterministic('estimated_γ1', V*Vbar)\n",
    "    γ2 = pm.Deterministic('estimated_γ2', P*N)\n",
    "          \n",
    "    # Reshape output of the model and get categorical likelihood\n",
    "    sigma = pm.HalfCauchy('sigma', 0.5)\n",
    "    likelihood = pm.Normal('likelihood', mu=estimated_overall_R, sigma=sigma, observed=pd.DataFrame(observed_R.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.variational.callbacks import CheckParametersConvergence\n",
    "with model:\n",
    "    approx = pm.fit(method='advi', n=40000, callbacks=[CheckParametersConvergence()])\n",
    "trace = approx.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_output = pm.summary(trace, kind='stats', varnames=[i for i in model.named_vars if 'α' in i and not i in model.deterministics and not 'log' in i and not 'interval' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, var_names=['αD_mean', 'αDE_mean', 'αDEF_mean', 'αS_mean', 'αE_mean', 'αF_mean', 'αEF_mean', 'αG_mean', 'αH_mean', 'αM_mean', 'αN_mean', 'αMN_mean', 'αU_mean', 'αUM_mean', 'αUMN_mean']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAIC, R2, and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_R_mean = trace['estimated_overall_R'].mean(axis=0)\n",
    "overall_R_sd = trace['estimated_overall_R'].std(axis=0)\n",
    "\n",
    "sub_ids = data['ID'].unique()\n",
    "\n",
    "subs = [np.where(data['ID'].unique() == sub)[0][0] for sub in sub_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2s = []\n",
    "\n",
    "for n, sub in enumerate(data['ID'].unique()):\n",
    "    r2s.append(r2_score(observed_R.squeeze()[~np.isnan(observed_R.squeeze()[:, n]), n], overall_R_mean[~np.isnan(observed_R.squeeze()[:, n]), n]))\n",
    "\n",
    "group_r2 = np.median(r2s)\n",
    "\n",
    "print(group_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2s = {'R2': [r2s]}\n",
    "\n",
    "r2s = pd.DataFrame(r2s)\n",
    "\n",
    "r2s.to_csv(os.path.join('../output/',r'2nd POS - OS2 Stimulus-Specific, R2 Output.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_output = pm.waic(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_output.to_csv(os.path.join('../output/',r'2nd POS - OS2 Stimulus-Specific, Alpha Output.csv'))\n",
    "waic_output.to_csv(os.path.join('../output/',r'2nd POS - OS2 Stimulus-Specific, WAIC Output.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Real, Model-Predicted, and Perfect Learning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('../data/', \"2nd POS Modeling All Data OS2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DV'] = ((data['DV'].values - 1) / 2) - 1\n",
    "\n",
    "observed_R_test = data.pivot(columns = 'ID', index = 'trialseq', values = 'DV').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the following cell, you will need to have run the \"Simulation Data\" code near the top of this notebook using \"Code for α = 1.\" This will allow you to re-create the three plots that are reported in the manuscript (as well as all other participants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(23, 3, figsize=(36, 48), dpi = 100)\n",
    "\n",
    "overall_R_mean = trace['estimated_overall_R'].mean(axis=0)\n",
    "overall_R_sd = trace['estimated_overall_R'].std(axis=0)\n",
    "\n",
    "sub_ids = data['ID'].unique()\n",
    "\n",
    "subs = [np.where(data['ID'].unique() == sub)[0][0] for sub in sub_ids]\n",
    "    \n",
    "for n, sub in enumerate(subs):\n",
    "    ax[n % 23, int(n / 23)].fill_between(range(overall_R_mean.shape[0]), overall_R_mean[:, sub] - overall_R_sd[:, sub], overall_R_mean[:, sub] + overall_R_sd[:, sub], alpha=0.3)\n",
    "    ax[n % 23, int(n / 23)].plot(overall_R_mean[:, sub])\n",
    "    ax[n % 23, int(n / 23)].plot(observed_R_test.squeeze()[:, sub], color='orange', linestyle='-')#participant's real data\n",
    "    ax[n % 23, int(n / 23)].plot(overall_R_sim.squeeze()[:, sub], color='silver', linestyle=':', alpha = .7)#Alpha = 1; this is the correct answer if a person learned perfectly\n",
    "    if n == 0:\n",
    "        ax[n % 23, int(n / 23)].set_ylabel('Mean (+/-SD) overall R')\n",
    "    ax[n % 23, int(n / 23)].set_ylabel('Responding (R)')\n",
    "    ax[n % 23, int(n / 23)].set_xlabel('Trials')\n",
    "    ax[n % 23, int(n / 23)].set_title('Sub {0}'.format(sub_ids[n]))   \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join('../output/',r'2nd POS - OS2 Stimulus-Specific, Individual Real and Estimated Responding.svg'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p conda,jupyterlab,numpy,pandas,theano,pymc3,sklearn,seaborn,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
