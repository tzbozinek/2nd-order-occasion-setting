{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from theano import scan\n",
    "import theano.tensor as T\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import os, sys, subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('../data/', \"2nd NOS Modeling Training Data OS1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DV'] = ((data['DV'].values - 1) / 2) - 1\n",
    "\n",
    "observed_R = data.pivot(columns = 'ID', index = 'trialseq', values = 'DV').values[:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occasion Setting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In its default form, the code block below allows λbar, γ1, and γ2 to scale per their formulas. However, in order to improve α parameter estimation (as described in our report), please set each of these to 1. Please see comments in code below on which code to activate and deactivate by highlighting that code and pressing ctrl / (or, on Mac, command /). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_function(stimuli_shown, Λ, λ, prev_V, prev_Vbar, prev_P, prev_N, stimulus_type, OS1_type, αA, αAB, αABC, αR, αB, αC, αBC, αG, αH, αJ, αK, αJK, αT, αTJ, αTJK):\n",
    "    \n",
    "    Λbar = T.zeros_like(Λ)\n",
    "    Λbar = T.inc_subtensor(Λbar[0,:], (prev_V[4,:] > 0) * (1 - Λ[0, :])) #Acs\n",
    "    Λbar = T.inc_subtensor(Λbar[1,:], (prev_V[2,:] > 0) * (1 - Λ[2, :])) #A1\n",
    "    Λbar = T.inc_subtensor(Λbar[2,:], (prev_V[2,:] > 0) * (1 - Λ[4, :])) #Bcs\n",
    "    Λbar = T.inc_subtensor(Λbar[3,:], (prev_V[4,:] > 0) * (1 - Λ[4, :])) #B1\n",
    "    Λbar = T.inc_subtensor(Λbar[4,:], (prev_V[4,:] > 0) * (1 - Λ[4, :])) #C\n",
    "    Λbar = T.inc_subtensor(Λbar[5,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #R\n",
    "    Λbar = T.inc_subtensor(Λbar[6,:], (prev_V[6,:] > 0) * (1 - Λ[6, :])) #G\n",
    "    Λbar = T.inc_subtensor(Λbar[7,:], (prev_V[6,:] > 0) * (1 - Λ[6, :])) #H\n",
    "    Λbar = T.inc_subtensor(Λbar[8,:], (prev_V[8,:] > 0) * (1 - Λ[10, :])) #Jcs\n",
    "    Λbar = T.inc_subtensor(Λbar[9,:], (prev_V[10,:] > 0) * (1 - Λ[10, :])) #J1\n",
    "    Λbar = T.inc_subtensor(Λbar[10,:], (prev_V[10,:] > 0) * (1 - Λ[10, :])) #K\n",
    "    Λbar = T.inc_subtensor(Λbar[11,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #Tcs\n",
    "    Λbar = T.inc_subtensor(Λbar[12,:], (prev_V[8,:] > 0) * (1 - Λ[9, :])) #T1\n",
    "    Λbar = T.inc_subtensor(Λbar[13,:], (prev_V[2,:] > 0) * (1 - Λ[2, :])) #A1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[14,:], (prev_V[4,:] > 0) * (1 - Λ[4, :])) #B1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[15,:], (prev_V[10,:] > 0) * (1 - Λ[10, :])) #J1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[16,:], (prev_V[8,:] > 0) * (1 - Λ[9, :])) #T1_abs\n",
    "    \n",
    "    #To make λbar = 1, activate \"λbar = T.ones_like(Λbar)\" code below and \n",
    "    #deactivate all \"λbar =\" code under \"Code for λbar Scaling\" below.\n",
    "    \n",
    "    #Code for λbar = 1\n",
    "#     λbar = T.ones_like(Λbar)\n",
    "    \n",
    "    #Code for λbar Scaling\n",
    "    λbar = T.zeros_like(Λbar)\n",
    "    λbar = T.inc_subtensor(λbar[0,:], prev_V[0,:]) #Acs\n",
    "    λbar = T.inc_subtensor(λbar[1,:], prev_V[2,:]) #A1\n",
    "    λbar = T.inc_subtensor(λbar[2,:], prev_V[4,:]) #Bcs\n",
    "    λbar = T.inc_subtensor(λbar[3,:], prev_V[4,:]) #B1\n",
    "    λbar = T.inc_subtensor(λbar[4,:], prev_V[4,:]) #C\n",
    "    λbar = T.inc_subtensor(λbar[5,:], prev_V[5,:]) #R\n",
    "    λbar = T.inc_subtensor(λbar[6,:], prev_V[6,:]) #G\n",
    "    λbar = T.inc_subtensor(λbar[7,:], prev_V[6,:]) #H\n",
    "    λbar = T.inc_subtensor(λbar[8,:], prev_V[10,:]) #Jcs\n",
    "    λbar = T.inc_subtensor(λbar[9,:], prev_V[10,:]) #J1\n",
    "    λbar = T.inc_subtensor(λbar[10,:], prev_V[10,:]) #K\n",
    "    λbar = T.inc_subtensor(λbar[11,:], prev_V[11,:]) #Tcs\n",
    "    λbar = T.inc_subtensor(λbar[12,:], prev_V[9,:]) #T1\n",
    "    λbar = T.inc_subtensor(λbar[13,:], prev_V[2,:]) #A1_abs\n",
    "    λbar = T.inc_subtensor(λbar[14,:], prev_V[4,:]) #B1_abs\n",
    "    λbar = T.inc_subtensor(λbar[15,:], prev_V[10,:]) #J1_abs\n",
    "    λbar = T.inc_subtensor(λbar[16,:], prev_V[8,:]) #T1_abs\n",
    "    \n",
    "    \n",
    "    #Prediction error\n",
    "    pe_V = λ - prev_V\n",
    "    pe_Vbar = λbar - prev_Vbar\n",
    "    pe_P = λ - prev_P\n",
    "    pe_N = λbar - prev_N\n",
    "    \n",
    "    #Stimulus-Specific α\n",
    "    αA = αA*(stimuli_shown[17,:] > 0)\n",
    "    αAB = αAB*(stimuli_shown[18,:] > 0)\n",
    "    αABC = αABC*(stimuli_shown[19,:] > 0)\n",
    "    αR = αR*(stimuli_shown[20,:] > 0)\n",
    "    αB = αB*(stimuli_shown[21,:] > 0)\n",
    "    αC = αC*(stimuli_shown[22,:] > 0)\n",
    "    αBC = αBC*(stimuli_shown[23,:] > 0)\n",
    "    αG = αG*(stimuli_shown[24,:] > 0)\n",
    "    αH = αH*(stimuli_shown[25,:] > 0)\n",
    "    αJ = αJ*(stimuli_shown[26,:] > 0)\n",
    "    αK = αK*(stimuli_shown[27,:] > 0)\n",
    "    αJK = αJK*(stimuli_shown[28,:] > 0)\n",
    "    αT = αT*(stimuli_shown[29,:] > 0)\n",
    "    αTJ = αTJ*(stimuli_shown[30,:] > 0)\n",
    "    αTJK = αTJK*(stimuli_shown[31,:] > 0)\n",
    "\n",
    "    #To make γ = 1, activate code under \"Code for γ = 1\" and \n",
    "    #deactivate code under \"Code for γ Scaling\" below.\n",
    "    \n",
    "    #Code for γ = 1\n",
    "#     prev_γ1 = T.zeros_like(prev_V)\n",
    "#     prev_γ1 = T.set_subtensor(prev_γ1[stimulus_type.nonzero(),:],1)\n",
    "    \n",
    "    #Code for γ Scaling\n",
    "    prev_γ1 = prev_V * prev_Vbar\n",
    "\n",
    "    #Delta formulas\n",
    "    CS_prev_γ1 = (prev_γ1 * stimuli_shown).sum(axis=0)\n",
    "    ΔV = Λ * (αA + αAB + αABC + αR + αB + αC + αBC + αG + αH + αJ + αK + αJK + αT + αTJ + αTJK) * pe_V\n",
    "    ΔVbar = Λbar * (αA + αAB + αABC + αR + αB + αC + αBC + αG + αH + αJ + αK + αJK + αT + αTJ + αTJK) * pe_Vbar\n",
    "    ΔP = Λ * (αA + αAB + αABC + αR + αB + αC + αBC + αG + αH + αJ + αK + αJK + αT + αTJ + αTJK) * CS_prev_γ1 * pe_P\n",
    "    ΔN = Λbar * (αA + αAB + αABC + αR + αB + αC + αBC + αG + αH + αJ + αK + αJK + αT + αTJ + αTJK) * CS_prev_γ1 * pe_N\n",
    "\n",
    "\n",
    "    # Only update stimuli that were shown\n",
    "    ΔV = ΔV * stimuli_shown\n",
    "    ΔVbar = ΔVbar * stimuli_shown\n",
    "    ΔP = ΔP * stimuli_shown\n",
    "    ΔN = ΔN * stimuli_shown\n",
    "    \n",
    "    # Update V, Vbar, P, N, P2, N2\n",
    "    V = T.zeros_like(prev_V)\n",
    "    Vbar = T.zeros_like(prev_Vbar)\n",
    "    P = T.zeros_like(prev_P)\n",
    "    N = T.zeros_like(prev_N)\n",
    "    \n",
    "    # Only update V and Vbar for CSs. Only update P and N for 1st-order OSs. Only update P2 and N2 for 2nd-order OSs.\n",
    "    V = T.inc_subtensor(V[T.eq(stimulus_type, 1)], prev_V[T.eq(stimulus_type, 1)] + ΔV[T.eq(stimulus_type, 1)])\n",
    "    Vbar = T.inc_subtensor(Vbar[T.eq(stimulus_type, 1)], prev_Vbar[T.eq(stimulus_type, 1)] + ΔVbar[T.eq(stimulus_type, 1)])\n",
    "    P = T.inc_subtensor(P[T.eq(OS1_type, 1)], prev_P[T.eq(OS1_type, 1)] + ΔP[T.eq(OS1_type, 1)])\n",
    "    N = T.inc_subtensor(N[T.eq(OS1_type, 1)], prev_N[T.eq(OS1_type, 1)] + ΔN[T.eq(OS1_type, 1)])\n",
    "    \n",
    "    return V, Vbar, P, N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Simulated Data with Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In code below, use \"Code for randomized α values\" to simulate random α values, which will then be estimated by our model. This was used for our simulated vs recovered plots in the manuscript. In order to graph \"perfect\" learning rate data, use \"Code for α = 1,\" which was plotted in our manuscript alongside participants with low, medium, and high α parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stim = 32\n",
    "n_subjects = len(data['ID'].unique())\n",
    "\n",
    "#Initial values\n",
    "R = np.zeros((n_stim, n_subjects))\n",
    "overall_R = np.zeros((1, n_subjects))\n",
    "v_excitatory = np.zeros((n_stim, n_subjects))\n",
    "v_inhibitory = np.zeros((n_stim, n_subjects))\n",
    "P = np.zeros((n_stim, n_subjects))\n",
    "N = np.zeros((n_stim, n_subjects))\n",
    "\n",
    "\n",
    "#Code for randomized α values\n",
    "gen_dist = pm.Beta.dist(2, 2, shape = n_subjects)\n",
    "αA_subject_sim = gen_dist.random()\n",
    "αAB_subject_sim = gen_dist.random()\n",
    "αABC_subject_sim = gen_dist.random()\n",
    "αR_subject_sim = gen_dist.random()\n",
    "αB_subject_sim = gen_dist.random()\n",
    "αC_subject_sim = gen_dist.random()\n",
    "αBC_subject_sim = gen_dist.random()\n",
    "αG_subject_sim = gen_dist.random()\n",
    "αH_subject_sim = gen_dist.random()\n",
    "αJ_subject_sim = gen_dist.random()\n",
    "αK_subject_sim = gen_dist.random()\n",
    "αJK_subject_sim = gen_dist.random()\n",
    "αT_subject_sim = gen_dist.random()\n",
    "αTJ_subject_sim = gen_dist.random()\n",
    "αTJK_subject_sim = gen_dist.random()\n",
    "\n",
    "\n",
    "#Code for α = 1\n",
    "# gen_dist = pm.Beta.dist(2, 2, shape=n_subjects)\n",
    "# α_subject_sim = np.ones(n_subjects)\n",
    "# αA_subject_sim = np.ones(n_subjects)\n",
    "# αAB_subject_sim = np.ones(n_subjects)\n",
    "# αABC_subject_sim = np.ones(n_subjects)\n",
    "# αR_subject_sim = np.ones(n_subjects)\n",
    "# αB_subject_sim = np.ones(n_subjects)\n",
    "# αC_subject_sim = np.ones(n_subjects)\n",
    "# αBC_subject_sim = np.ones(n_subjects)\n",
    "# αG_subject_sim = np.ones(n_subjects)\n",
    "# αH_subject_sim = np.ones(n_subjects)\n",
    "# αJ_subject_sim = np.ones(n_subjects)\n",
    "# αK_subject_sim = np.ones(n_subjects)\n",
    "# αJK_subject_sim = np.ones(n_subjects)\n",
    "# αT_subject_sim = np.ones(n_subjects)\n",
    "# αTJ_subject_sim = np.ones(n_subjects)\n",
    "# αTJK_subject_sim = np.ones(n_subjects)\n",
    "\n",
    "#US values\n",
    "small_lambda = data.pivot(index='trialseq', values='US', columns='ID').values[:, np.newaxis, :].repeat(n_stim, axis=1).astype(float)\n",
    "stim_data = []\n",
    "\n",
    "for sub in data['ID'].unique():\n",
    "    stim_data.append(data.loc[data['ID'] == sub, ['Acs', 'A1', 'Bcs', 'B1', 'C', 'R', 'G', 'H', 'Jcs', \n",
    "                                                  'J1', 'K', 'Tcs', 'T1', 'A1_abs', 'B1_abs', \n",
    "                                                  'J1_abs', 'T1_abs',\n",
    "                                                  'alpha_A', 'alpha_AB', 'alpha_ABC', 'alpha_R', 'alpha_B', 'alpha_C',\n",
    "                                                  'alpha_BC', 'alpha_G', 'alpha_H', 'alpha_J', 'alpha_K', 'alpha_JK',\n",
    "                                                  'alpha_T', 'alpha_TJ', 'alpha_TJK']].values)\n",
    "\n",
    "stimuli_shown = np.dstack(stim_data)\n",
    "big_lambda = small_lambda\n",
    "\n",
    "#Add imaginary -1th trial\n",
    "big_lambda = np.vstack([np.zeros((1, n_stim, n_subjects)), big_lambda[:-1, ...]]).astype(float) # Add one trial of zeros to the start, remove the last trial\n",
    "small_lambda = big_lambda\n",
    "stimuli_shown = np.vstack([np.zeros((1, n_stim, n_subjects)), stimuli_shown]) # Add one trial of zeros to the start, DO NOT remove the last trial - this is needed for prediction\n",
    "\n",
    "#Designate stimulus types\n",
    "stimulus_type = np.ones(n_stim)\n",
    "stimulus_type[[1, 3, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]] = 0 #make all OSs = 0\n",
    "\n",
    "OS1_type = np.zeros(n_stim)\n",
    "OS1_type[[1, 3, 9, 12, 13, 14, 15, 16]] = 1 #make 1st OSs = 1\n",
    "\n",
    "\n",
    "#Convert task outcomes to tensors\n",
    "big_lambda = T.as_tensor_variable(big_lambda.astype(float))\n",
    "small_lambda = T.as_tensor_variable(small_lambda.astype(float))\n",
    "stimuli_shown = T.as_tensor_variable(stimuli_shown)\n",
    "\n",
    "stimuli_shown_sim = stimuli_shown.copy()\n",
    "big_lambda_sim = big_lambda.copy()\n",
    "small_lambda_sim = small_lambda.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Fake Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the loop\n",
    "output, updates = scan(fn=learning_function,\n",
    "                    sequences=[{'input': stimuli_shown_sim[:-1, ...]},\n",
    "                             {'input': big_lambda_sim},\n",
    "                             {'input': small_lambda_sim}],\n",
    "                    outputs_info=[v_excitatory, v_inhibitory, P, N],\n",
    "                    non_sequences = [stimulus_type, OS1_type, αA_subject_sim, αAB_subject_sim, αABC_subject_sim, αR_subject_sim, αB_subject_sim, αC_subject_sim, αBC_subject_sim, αG_subject_sim, αH_subject_sim, αJ_subject_sim, αK_subject_sim, αJK_subject_sim, αT_subject_sim, αTJ_subject_sim, αTJK_subject_sim])\n",
    "\n",
    "#Get model output\n",
    "V_out, Vbar_out, P_out, N_out = [i.eval() for i in output]\n",
    "\n",
    "estimated_overall_R = ((V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) - (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) + \\\n",
    "    ((P_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) - \\\n",
    "    ((N_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1))\n",
    "\n",
    "overall_R_sim = estimated_overall_R.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = len(data['ID'].unique())\n",
    "\n",
    "#Initial values\n",
    "R = np.zeros((n_stim, n_subjects))\n",
    "\n",
    "#US values\n",
    "small_lambda = data.pivot(index='trialseq', values='US', columns='ID').values[:, np.newaxis, :].repeat(n_stim, axis=1).astype(float)\n",
    "stim_data = []\n",
    "\n",
    "for sub in data['ID'].unique():\n",
    "    stim_data.append(data.loc[data['ID'] == sub, ['Acs', 'A1', 'Bcs', 'B1', 'C', 'R', 'G', 'H', 'Jcs', \n",
    "                                                  'J1', 'K', 'Tcs', 'T1', 'A1_abs', 'B1_abs', \n",
    "                                                  'J1_abs', 'T1_abs',\n",
    "                                                  'alpha_A', 'alpha_AB', 'alpha_ABC', 'alpha_R', 'alpha_B', 'alpha_C',\n",
    "                                                  'alpha_BC', 'alpha_G', 'alpha_H', 'alpha_J', 'alpha_K', 'alpha_JK',\n",
    "                                                  'alpha_T', 'alpha_TJ', 'alpha_TJK']].values)\n",
    "\n",
    "stimuli_shown = np.dstack(stim_data)\n",
    "big_lambda = small_lambda\n",
    "\n",
    "#Add imaginary -1th trial\n",
    "big_lambda = np.vstack([np.zeros((1, n_stim, n_subjects)), big_lambda[:-1, ...]]).astype(float) # Add one trial of zeros to the start, remove the last trial\n",
    "small_lambda = big_lambda\n",
    "stimuli_shown = np.vstack([np.zeros((1, n_stim, n_subjects)), stimuli_shown]) # Add one trial of zeros to the start, DO NOT remove the last trial - this is needed for prediction\n",
    "\n",
    "stimulus_type = np.ones(n_stim)\n",
    "stimulus_type[[1, 3, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]] = 0 #make all OSs = 0\n",
    "\n",
    "OS1_type = np.zeros(n_stim)\n",
    "OS1_type[[1, 3, 9, 12, 13, 14, 15, 16]] = 1 #make 1st OSs = 1\n",
    "\n",
    "#Convert task outcomes to tensors\n",
    "big_lambda = T.as_tensor_variable(big_lambda.astype(float))\n",
    "small_lambda = T.as_tensor_variable(small_lambda.astype(float))\n",
    "stimuli_shown = T.as_tensor_variable(stimuli_shown)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    \n",
    "    # Learning rate lies between 0 and 1, so we use a beta distribution\n",
    "    αA_mean = pm.Normal('αA_mean', 0.5, 10)\n",
    "    αA_sd = pm.HalfCauchy('αA_sd', 10)\n",
    "    \n",
    "    αAB_mean = pm.Normal('αAB_mean', 0.5, 10)\n",
    "    αAB_sd = pm.HalfCauchy('αAB_sd', 10)\n",
    "    \n",
    "    αABC_mean = pm.Normal('αABC_mean', 0.5, 10)\n",
    "    αABC_sd = pm.HalfCauchy('αABC_sd', 10)\n",
    "    \n",
    "    αR_mean = pm.Normal('αR_mean', 0.5, 10)\n",
    "    αR_sd = pm.HalfCauchy('αR_sd', 10)\n",
    "    \n",
    "    αB_mean = pm.Normal('αB_mean', 0.5, 10)\n",
    "    αB_sd = pm.HalfCauchy('αB_sd', 10)\n",
    "    \n",
    "    αC_mean = pm.Normal('αC_mean', 0.5, 10)\n",
    "    αC_sd = pm.HalfCauchy('αC_sd', 10)\n",
    "    \n",
    "    αBC_mean = pm.Normal('αBC_mean', 0.5, 10)\n",
    "    αBC_sd = pm.HalfCauchy('αBC_sd', 10)\n",
    "    \n",
    "    αG_mean = pm.Normal('αG_mean', 0.5, 10)\n",
    "    αG_sd = pm.HalfCauchy('αG_sd', 10)\n",
    "    \n",
    "    αH_mean = pm.Normal('αH_mean', 0.5, 10)\n",
    "    αH_sd = pm.HalfCauchy('αH_sd', 10)\n",
    "    \n",
    "    αJ_mean = pm.Normal('αJ_mean', 0.5, 10)\n",
    "    αJ_sd = pm.HalfCauchy('αJ_sd', 10)\n",
    "    \n",
    "    αK_mean = pm.Normal('αK_mean', 0.5, 10)\n",
    "    αK_sd = pm.HalfCauchy('αK_sd', 10)\n",
    "    \n",
    "    αJK_mean = pm.Normal('αJK_mean', 0.5, 10)\n",
    "    αJK_sd = pm.HalfCauchy('αJK_sd', 10)\n",
    "    \n",
    "    αT_mean = pm.Normal('αT_mean', 0.5, 10)\n",
    "    αT_sd = pm.HalfCauchy('αT_sd', 10)\n",
    "    \n",
    "    αTJ_mean = pm.Normal('αTJ_mean', 0.5, 10)\n",
    "    αTJ_sd = pm.HalfCauchy('αTJ_sd', 10)\n",
    "    \n",
    "    αTJK_mean = pm.Normal('αTJK_mean', 0.5, 10)\n",
    "    αTJK_sd = pm.HalfCauchy('αTJK_sd', 10)\n",
    "\n",
    "    \n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=0, upper=1)\n",
    "    αA_subject = BoundedNormal('αA', mu=αA_mean, sd=αA_sd, shape=(n_subjects,))\n",
    "    αAB_subject = BoundedNormal('αAB', mu=αAB_mean, sd=αAB_sd, shape=(n_subjects,))\n",
    "    αABC_subject = BoundedNormal('αABC', mu=αABC_mean, sd=αABC_sd, shape=(n_subjects,))\n",
    "    αR_subject = BoundedNormal('αR', mu=αR_mean, sd=αR_sd, shape=(n_subjects,))\n",
    "    αB_subject = BoundedNormal('αB', mu=αB_mean, sd=αB_sd, shape=(n_subjects,))\n",
    "    αC_subject = BoundedNormal('αC', mu=αC_mean, sd=αC_sd, shape=(n_subjects,))\n",
    "    αBC_subject = BoundedNormal('αBC', mu=αBC_mean, sd=αBC_sd, shape=(n_subjects,))\n",
    "    αG_subject = BoundedNormal('αG', mu=αG_mean, sd=αG_sd, shape=(n_subjects,))\n",
    "    αH_subject = BoundedNormal('αH', mu=αH_mean, sd=αH_sd, shape=(n_subjects,))\n",
    "    αJ_subject = BoundedNormal('αJ', mu=αJ_mean, sd=αJ_sd, shape=(n_subjects,))\n",
    "    αK_subject = BoundedNormal('αK', mu=αK_mean, sd=αK_sd, shape=(n_subjects,))\n",
    "    αJK_subject = BoundedNormal('αJK', mu=αJK_mean, sd=αJK_sd, shape=(n_subjects,))\n",
    "    αT_subject = BoundedNormal('αT', mu=αT_mean, sd=αT_sd, shape=(n_subjects,))\n",
    "    αTJ_subject = BoundedNormal('αTJ', mu=αTJ_mean, sd=αTJ_sd, shape=(n_subjects,))\n",
    "    αTJK_subject = BoundedNormal('αTJK', mu=αTJK_mean, sd=αTJK_sd, shape=(n_subjects,))\n",
    "\n",
    "    \n",
    "# Run the loop\n",
    "    output, updates = scan(fn=learning_function,\n",
    "                      sequences=[dict(input=stimuli_shown[:-1, ...]), dict(input=big_lambda), dict(input=small_lambda)],\n",
    "                      outputs_info=[v_excitatory, v_inhibitory, P, N],\n",
    "                      non_sequences = [stimulus_type, OS1_type, αA_subject, αAB_subject, αABC_subject, αR_subject, αB_subject, αC_subject, αBC_subject, αG_subject, αH_subject, αJ_subject, αK_subject, αJK_subject, αT_subject, αTJ_subject, αTJK_subject])\n",
    "    \n",
    "    # Get model output\n",
    "    V, Vbar, P, N = output\n",
    "    \n",
    "    # Calculate response - combine value learning and occasion setting\n",
    "    R = (V - Vbar) + ((P * Vbar * V*Vbar) - (N * V *V*Vbar))\n",
    "\n",
    "    # # Single R value\n",
    "    estimated_overall_R = ((V * stimuli_shown[1:, ...]).sum(axis=1) - (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1))\n",
    "   \n",
    "    # This allows us to output the estimated R\n",
    "    estimated_overall_R = pm.Deterministic('estimated_overall_R', estimated_overall_R)\n",
    "    \n",
    "    # Reshape output of the model and get categorical likelihood\n",
    "    sigma = pm.HalfCauchy('sigma', 0.5)\n",
    "    likelihood = pm.Normal('likelihood', mu=estimated_overall_R, sigma=sigma, observed=pd.DataFrame(overall_R_sim.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Data (go to \"Real Data\" below if you want to skip data simulations and just get real data results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.variational.callbacks import CheckParametersConvergence\n",
    "with model:\n",
    "    approx = pm.fit(method='advi', n=40000, callbacks=[CheckParametersConvergence()])\n",
    "trace = approx.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_output = pm.summary(trace, kind='stats', varnames=[i for i in model.named_vars if 'α' in i and not i in model.deterministics and not 'log' in i and not 'interval' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, var_names=['αA_mean', 'αAB_mean', 'αABC_mean', 'αR_mean', 'αB_mean', 'αC_mean', 'αBC_mean', 'αG_mean', 'αH_mean', 'αJ_mean', 'αK_mean', 'αJK_mean', 'αT_mean', 'αTJ_mean', 'αTJK_mean']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_data_var = {'Simulated_αA': αA_subject_sim, 'Recovered_αA': trace['αA'].mean(axis=0), \n",
    "                      'Simulated_αAB': αAB_subject_sim, 'Recovered_αAB': trace['αAB'].mean(axis=0),\n",
    "                      'Simulated_αABC': αABC_subject_sim, 'Recovered_αABC': trace['αABC'].mean(axis=0),\n",
    "                      'Simulated_αR': αR_subject_sim, 'Recovered_αR': trace['αR'].mean(axis=0),\n",
    "                      'Simulated_αB': αB_subject_sim, 'Recovered_αB': trace['αB'].mean(axis=0),\n",
    "                      'Simulated_αC': αC_subject_sim, 'Recovered_αC': trace['αC'].mean(axis=0),\n",
    "                      'Simulated_αBC': αBC_subject_sim, 'Recovered_αBC': trace['αBC'].mean(axis=0),\n",
    "                      'Simulated_αG': αG_subject_sim, 'Recovered_αG': trace['αG'].mean(axis=0),\n",
    "                      'Simulated_αH': αH_subject_sim, 'Recovered_αH': trace['αH'].mean(axis=0),\n",
    "                      'Simulated_αJ': αJ_subject_sim, 'Recovered_αJ': trace['αJ'].mean(axis=0),\n",
    "                      'Simulated_αK': αK_subject_sim, 'Recovered_αK': trace['αK'].mean(axis=0),\n",
    "                      'Simulated_αJK': αJK_subject_sim, 'Recovered_αJK': trace['αJK'].mean(axis=0),\n",
    "                      'Simulated_αT': αT_subject_sim, 'Recovered_αT': trace['αT'].mean(axis=0),\n",
    "                      'Simulated_αTJ': αTJ_subject_sim, 'Recovered_αTJ': trace['αTJ'].mean(axis=0),\n",
    "                      'Simulated_αTJK': αTJK_subject_sim, 'Recovered_αTJK': trace['αTJK'].mean(axis=0)}\n",
    "\n",
    "recovered_data_var = pd.DataFrame(recovered_data_var)\n",
    "recovered_data_var.to_csv(os.path.join('../output/',r'2nd NOS - OS1 Stimulus-Specific, Simulated vs Recovered.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(3, 5, sharex = True, sharey = True, figsize=(12, 7.5))\n",
    "f.suptitle('Simulated vs Recovered α Parameters', y=1.02, fontsize = 16)\n",
    "f.text(.5, -.02, 'Simulated α', va='center', ha='center', fontsize = 16)\n",
    "f.text(-.02, .5, 'Recovered α', va='center', ha='center', fontsize = 16, rotation=90)\n",
    "\n",
    "sns.regplot(αA_subject_sim, trace['αA'].mean(axis=0), label='αA_subject', ax=ax[0,0], color = 'black')\n",
    "sns.regplot(αAB_subject_sim, trace['αAB'].mean(axis=0), label='αAB_subject', ax=ax[0,1], color = 'black')\n",
    "sns.regplot(αABC_subject_sim, trace['αABC'].mean(axis=0), label='αABC_subject', ax=ax[0,2], color = 'black')\n",
    "sns.regplot(αR_subject_sim, trace['αR'].mean(axis=0), label='αR_subject', ax=ax[0,3], color = 'black')\n",
    "sns.regplot(αB_subject_sim, trace['αB'].mean(axis=0), label='αB_subject', ax=ax[0,4], color = 'black')\n",
    "sns.regplot(αC_subject_sim, trace['αC'].mean(axis=0), label='αC_subject', ax=ax[1,0], color = 'black')\n",
    "sns.regplot(αBC_subject_sim, trace['αBC'].mean(axis=0), label='αBC_subject', ax=ax[1,1], color = 'black')\n",
    "sns.regplot(αG_subject_sim, trace['αG'].mean(axis=0), label='αG_subject', ax=ax[1,2], color = 'black')\n",
    "sns.regplot(αH_subject_sim, trace['αH'].mean(axis=0), label='αH_subject', ax=ax[1,3], color = 'black')\n",
    "sns.regplot(αJ_subject_sim, trace['αJ'].mean(axis=0), label='αJ_subject', ax=ax[1,4], color = 'black')\n",
    "sns.regplot(αK_subject_sim, trace['αK'].mean(axis=0), label='αK_subject', ax=ax[2,0], color = 'black')\n",
    "sns.regplot(αJK_subject_sim, trace['αJK'].mean(axis=0), label='αJK_subject', ax=ax[2,1], color = 'black')\n",
    "sns.regplot(αT_subject_sim, trace['αT'].mean(axis=0), label='αT_subject', ax=ax[2,2], color = 'black')\n",
    "sns.regplot(αTJ_subject_sim, trace['αTJ'].mean(axis=0), label='αTJ_subject', ax=ax[2,3], color = 'black')\n",
    "sns.regplot(αTJK_subject_sim, trace['αTJK'].mean(axis=0), label='αTJK_subject', ax=ax[2,4], color = 'black')\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        ax[0,0].set_title('α A')\n",
    "        ax[0,1].set_title('α AB')\n",
    "        ax[0,2].set_title('α ABC')\n",
    "        ax[0,3].set_title('α R')\n",
    "        ax[0,4].set_title('α B')\n",
    "        ax[1,0].set_title('α C')\n",
    "        ax[1,1].set_title('α BC')\n",
    "        ax[1,2].set_title('α G')\n",
    "        ax[1,3].set_title('α H')\n",
    "        ax[1,4].set_title('α J')\n",
    "        ax[2,0].set_title('α K')\n",
    "        ax[2,1].set_title('α JK')\n",
    "        ax[2,2].set_title('α T')\n",
    "        ax[2,3].set_title('α TJ')\n",
    "        ax[2,4].set_title('α TJK')\n",
    "\n",
    "plt.setp(ax, xticks=[0, .2, .4, .6, .8, 1], yticks=[0, .2, .4, .6, .8, 1])        \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join('../output/',r'2nd NOS - OS1 Stimulus-Specific, Simulated vs Recovered.svg'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model to Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = len(data['ID'].unique())\n",
    "\n",
    "# Initial values\n",
    "R = np.zeros((n_stim, n_subjects))  # Value estimate\n",
    "overall_R = np.zeros((1, n_subjects))\n",
    "v_excitatory = np.zeros((n_stim, n_subjects)) \n",
    "v_inhibitory = np.zeros((n_stim, n_subjects)) \n",
    "P = np.zeros((n_stim, n_subjects))\n",
    "N = np.zeros((n_stim, n_subjects))\n",
    "P2 = np.zeros((n_stim, n_subjects))\n",
    "N2 = np.zeros((n_stim, n_subjects))\n",
    "gamma = np.ones((n_stim, n_subjects))\n",
    "\n",
    "# US values\n",
    "small_lambda = data.pivot(index='trialseq', values='US', columns='ID').values[:, np.newaxis, :].repeat(n_stim, axis=1)\n",
    "stim_data = []\n",
    "\n",
    "for sub in data['ID'].unique():\n",
    "    stim_data.append(data.loc[data['ID'] == sub, ['Acs', 'A1', 'Bcs', 'B1', 'C', 'R', 'G', 'H', 'Jcs', \n",
    "                                                  'J1', 'K', 'Tcs', 'T1', 'A1_abs', 'B1_abs', \n",
    "                                                  'J1_abs', 'T1_abs',\n",
    "                                                  'alpha_A', 'alpha_AB', 'alpha_ABC', 'alpha_R', 'alpha_B', 'alpha_C',\n",
    "                                                  'alpha_BC', 'alpha_G', 'alpha_H', 'alpha_J', 'alpha_K', 'alpha_JK',\n",
    "                                                  'alpha_T', 'alpha_TJ', 'alpha_TJK']].values)\n",
    "    \n",
    "stimuli_shown = np.dstack(stim_data)\n",
    "big_lambda = small_lambda\n",
    "\n",
    "# Add imaginary -1th trial\n",
    "big_lambda = np.vstack([np.zeros((1, n_stim, n_subjects)), big_lambda[:-1, ...]])  # Add one trial of zeros to the start, remove the last trial\n",
    "small_lambda = big_lambda\n",
    "stimuli_shown = np.vstack([np.zeros((1, n_stim, n_subjects)), stimuli_shown]) # Add one trial of zeros to the start, DO NOT remove the last trial - this is needed for prediction\n",
    "\n",
    "stimulus_type = np.ones(n_stim)\n",
    "stimulus_type[[1, 3, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]] = 0 #make all OSs = 0\n",
    "\n",
    "OS1_type = np.zeros(n_stim)\n",
    "OS1_type[[1, 3, 9, 12, 13, 14, 15, 16]] = 1 #make 1st OSs = 1\n",
    "\n",
    "# Convert task outcomes to tensors\n",
    "big_lambda = T.as_tensor_variable(big_lambda)\n",
    "small_lambda = T.as_tensor_variable(small_lambda)\n",
    "stimuli_shown = T.as_tensor_variable(stimuli_shown)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    \n",
    "    # Learning rate lies between 0 and 1 so we use a beta distribution\n",
    "    αA_mean = pm.Normal('αA_mean', 0.5, 10)\n",
    "    αA_sd = pm.HalfCauchy('αA_sd', 10)\n",
    "    \n",
    "    αAB_mean = pm.Normal('αAB_mean', 0.5, 10)\n",
    "    αAB_sd = pm.HalfCauchy('αAB_sd', 10)\n",
    "    \n",
    "    αABC_mean = pm.Normal('αABC_mean', 0.5, 10)\n",
    "    αABC_sd = pm.HalfCauchy('αABC_sd', 10)\n",
    "    \n",
    "    αR_mean = pm.Normal('αR_mean', 0.5, 10)\n",
    "    αR_sd = pm.HalfCauchy('αR_sd', 10)\n",
    "    \n",
    "    αB_mean = pm.Normal('αB_mean', 0.5, 10)\n",
    "    αB_sd = pm.HalfCauchy('αB_sd', 10)\n",
    "    \n",
    "    αC_mean = pm.Normal('αC_mean', 0.5, 10)\n",
    "    αC_sd = pm.HalfCauchy('αC_sd', 10)\n",
    "    \n",
    "    αBC_mean = pm.Normal('αBC_mean', 0.5, 10)\n",
    "    αBC_sd = pm.HalfCauchy('αBC_sd', 10)\n",
    "    \n",
    "    αG_mean = pm.Normal('αG_mean', 0.5, 10)\n",
    "    αG_sd = pm.HalfCauchy('αG_sd', 10)\n",
    "    \n",
    "    αH_mean = pm.Normal('αH_mean', 0.5, 10)\n",
    "    αH_sd = pm.HalfCauchy('αH_sd', 10)\n",
    "    \n",
    "    αJ_mean = pm.Normal('αJ_mean', 0.5, 10)\n",
    "    αJ_sd = pm.HalfCauchy('αJ_sd', 10)\n",
    "    \n",
    "    αK_mean = pm.Normal('αK_mean', 0.5, 10)\n",
    "    αK_sd = pm.HalfCauchy('αK_sd', 10)\n",
    "    \n",
    "    αJK_mean = pm.Normal('αJK_mean', 0.5, 10)\n",
    "    αJK_sd = pm.HalfCauchy('αJK_sd', 10)\n",
    "    \n",
    "    αT_mean = pm.Normal('αT_mean', 0.5, 10)\n",
    "    αT_sd = pm.HalfCauchy('αT_sd', 10)\n",
    "    \n",
    "    αTJ_mean = pm.Normal('αTJ_mean', 0.5, 10)\n",
    "    αTJ_sd = pm.HalfCauchy('αTJ_sd', 10)\n",
    "    \n",
    "    αTJK_mean = pm.Normal('αTJK_mean', 0.5, 10)\n",
    "    αTJK_sd = pm.HalfCauchy('αTJK_sd', 10)\n",
    "\n",
    "    \n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=0, upper=1)\n",
    "    αA_subject = BoundedNormal('αA', mu=αA_mean, sd=αA_sd, shape=(n_subjects,))\n",
    "    αAB_subject = BoundedNormal('αAB', mu=αAB_mean, sd=αAB_sd, shape=(n_subjects,))\n",
    "    αABC_subject = BoundedNormal('αABC', mu=αABC_mean, sd=αABC_sd, shape=(n_subjects,))\n",
    "    αR_subject = BoundedNormal('αR', mu=αR_mean, sd=αR_sd, shape=(n_subjects,))\n",
    "    αB_subject = BoundedNormal('αB', mu=αB_mean, sd=αB_sd, shape=(n_subjects,))\n",
    "    αC_subject = BoundedNormal('αC', mu=αC_mean, sd=αC_sd, shape=(n_subjects,))\n",
    "    αBC_subject = BoundedNormal('αBC', mu=αBC_mean, sd=αBC_sd, shape=(n_subjects,))\n",
    "    αG_subject = BoundedNormal('αG', mu=αG_mean, sd=αG_sd, shape=(n_subjects,))\n",
    "    αH_subject = BoundedNormal('αH', mu=αH_mean, sd=αH_sd, shape=(n_subjects,))\n",
    "    αJ_subject = BoundedNormal('αJ', mu=αJ_mean, sd=αJ_sd, shape=(n_subjects,))\n",
    "    αK_subject = BoundedNormal('αK', mu=αK_mean, sd=αK_sd, shape=(n_subjects,))\n",
    "    αJK_subject = BoundedNormal('αJK', mu=αJK_mean, sd=αJK_sd, shape=(n_subjects,))\n",
    "    αT_subject = BoundedNormal('αT', mu=αT_mean, sd=αT_sd, shape=(n_subjects,))\n",
    "    αTJ_subject = BoundedNormal('αTJ', mu=αTJ_mean, sd=αTJ_sd, shape=(n_subjects,))\n",
    "    αTJK_subject = BoundedNormal('αTJK', mu=αTJK_mean, sd=αTJK_sd, shape=(n_subjects,))\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # Run the loop\n",
    "    output, updates = scan(fn=learning_function,\n",
    "                      sequences=[dict(input=stimuli_shown[:-1, ...]), dict(input=big_lambda), dict(input=small_lambda)],\n",
    "                      outputs_info=[v_excitatory, v_inhibitory, P, N],\n",
    "                      non_sequences = [stimulus_type, OS1_type, αA_subject, αAB_subject, αABC_subject, αR_subject, αB_subject, αC_subject, αBC_subject, αG_subject, αH_subject, αJ_subject, αK_subject, αJK_subject, αT_subject, αTJ_subject, αTJK_subject])\n",
    "    \n",
    "    # Get model output\n",
    "    V, Vbar, P, N = output\n",
    "    \n",
    "    # Calculate response - combine value learning and occasion setting\n",
    "    R = (V - Vbar) + ((P * Vbar * V*Vbar) - (N * V *V*Vbar))\n",
    "\n",
    "    # # Single R value\n",
    "    estimated_overall_R = ((V * stimuli_shown[1:, ...]).sum(axis=1) - (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1))\n",
    "    \n",
    "    # This allows us to output the estimated R\n",
    "    estimated_overall_R = pm.Deterministic('estimated_overall_R', estimated_overall_R)\n",
    "    V = pm.Deterministic('estimated_V', V)\n",
    "    Vbar = pm.Deterministic('estimated_Vbar', Vbar)\n",
    "    P = pm.Deterministic('estimated_P', P)\n",
    "    N = pm.Deterministic('estimated_N', N)\n",
    "    γ1 = pm.Deterministic('estimated_γ1', V*Vbar)\n",
    "          \n",
    "    # Reshape output of the model and get categorical likelihood\n",
    "    sigma = pm.HalfCauchy('sigma', 0.5)\n",
    "    likelihood = pm.Normal('likelihood', mu=estimated_overall_R, sigma=sigma, observed=pd.DataFrame(observed_R.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.variational.callbacks import CheckParametersConvergence\n",
    "with model:\n",
    "    approx = pm.fit(method='advi', n=40000, callbacks=[CheckParametersConvergence()])\n",
    "trace = approx.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_output = pm.summary(trace, kind='stats', varnames=[i for i in model.named_vars if 'α' in i and not i in model.deterministics and not 'log' in i and not 'interval' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, var_names=['αA_mean', 'αAB_mean', 'αABC_mean', 'αR_mean', 'αB_mean', 'αC_mean', 'αBC_mean', 'αG_mean', 'αH_mean', 'αJ_mean', 'αK_mean', 'αJK_mean', 'αT_mean', 'αTJ_mean', 'αTJK_mean']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAIC, R2, and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_R_mean = trace['estimated_overall_R'].mean(axis=0)\n",
    "overall_R_sd = trace['estimated_overall_R'].std(axis=0)\n",
    "\n",
    "sub_ids = data['ID'].unique()\n",
    "\n",
    "subs = [np.where(data['ID'].unique() == sub)[0][0] for sub in sub_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2s = []\n",
    "\n",
    "for n, sub in enumerate(data['ID'].unique()):\n",
    "    r2s.append(r2_score(observed_R.squeeze()[~np.isnan(observed_R.squeeze()[:, n]), n], overall_R_mean[~np.isnan(observed_R.squeeze()[:, n]), n]))\n",
    "\n",
    "group_r2 = np.median(r2s)\n",
    "\n",
    "print(group_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2s = {'R2': [r2s]}\n",
    "\n",
    "r2s = pd.DataFrame(r2s)\n",
    "\n",
    "r2s.to_csv(os.path.join('../output/',r'2nd NOS - OS1 Stimulus-Specific, R2 Output.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_output = pm.waic(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_output.to_csv(os.path.join('../output/',r'2nd NOS - OS1 Stimulus-Specific, Alpha Output.csv'))\n",
    "waic_output.to_csv(os.path.join('../output/',r'2nd NOS - OS1 Stimulus-Specific, WAIC Output.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Real, Model-Predicted, and Perfect Learning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('../data/', \"2nd NOS Modeling All Data OS1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DV'] = ((data['DV'].values - 1) / 2) - 1\n",
    "\n",
    "observed_R_test = data.pivot(columns = 'ID', index = 'trialseq', values = 'DV').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the following cell, you will need to have run the \"Simulation Data\" code near the top of this notebook using \"Code for α = 1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(20, 3, figsize=(36, 48), dpi = 100)\n",
    "\n",
    "overall_R_mean = trace['estimated_overall_R'].mean(axis=0)\n",
    "overall_R_sd = trace['estimated_overall_R'].std(axis=0)\n",
    "\n",
    "sub_ids = data['ID'].unique()\n",
    "\n",
    "subs = [np.where(data['ID'].unique() == sub)[0][0] for sub in sub_ids]\n",
    "    \n",
    "for n, sub in enumerate(subs):\n",
    "    ax[n % 20, int(n / 20)].fill_between(range(overall_R_mean.shape[0]), overall_R_mean[:, sub] - overall_R_sd[:, sub], overall_R_mean[:, sub] + overall_R_sd[:, sub], alpha=0.3)\n",
    "    ax[n % 20, int(n / 20)].plot(overall_R_mean[:, sub])\n",
    "    ax[n % 20, int(n / 20)].plot(observed_R_test.squeeze()[:, sub], color='orange', linestyle='-')#participant's real data\n",
    "    ax[n % 20, int(n / 20)].plot(overall_R_sim.squeeze()[:, sub], color='grey', linestyle=':', alpha = .7)#Alpha = 1; this is the correct answer if a person learned perfectly\n",
    "    if n == 0:\n",
    "        ax[n % 20, int(n / 20)].set_ylabel('Mean (+/-SD) overall R')\n",
    "    ax[n % 20, int(n / 20)].set_ylabel('Responding (R)')\n",
    "    ax[n % 20, int(n / 20)].set_xlabel('Trials')\n",
    "    ax[n % 20, int(n / 20)].set_title('Sub {0}'.format(sub_ids[n]))   \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join('../output/',r'2nd NOS - OS1 Stimulus-Specific, Individual Real and Estimated Responding.svg'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p conda,jupyterlab,numpy,pandas,theano,pymc3,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
