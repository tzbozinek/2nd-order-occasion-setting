{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from theano import scan\n",
    "import theano.tensor as T\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import os, sys, subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('../data/', \"2nd POS Modeling Training Data OS2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DV'] = ((data['DV'].values - 1) / 2) - 1\n",
    "\n",
    "observed_R = data.pivot(columns = 'ID', index = 'trialseq', values = 'DV').values[:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occasion Setting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In its default form, the code block below allows λbar, γ1, and γ2 to scale per their formulas. However, in order to improve α parameter estimation (as described in our report), please set each of these to 1. Please see comments in code below on which code to activate and deactivate by highlighting that code and pressing ctrl / (or, on Mac, command /). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_function(stimuli_shown, Λ, λ, training_or_test, prev_V, prev_Vbar, prev_P, prev_N, prev_P2, prev_N2, stimulus_type, OS1_type, OS2_type, α):\n",
    "    \n",
    "    Λbar = T.zeros_like(Λ)\n",
    "    Λbar = T.inc_subtensor(Λbar[0,:], (prev_V[5,:] > 0) * (1 - Λ[0, :])) #Dcs\n",
    "    Λbar = T.inc_subtensor(Λbar[1,:], (prev_V[3,:] > 0) * (1 - Λ[3, :])) #D1\n",
    "    Λbar = T.inc_subtensor(Λbar[2,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #D2\n",
    "    Λbar = T.inc_subtensor(Λbar[3,:], (prev_V[3,:] > 0) * (1 - Λ[3, :])) #Ecs\n",
    "    Λbar = T.inc_subtensor(Λbar[4,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #E1\n",
    "    Λbar = T.inc_subtensor(Λbar[5,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #F\n",
    "    Λbar = T.inc_subtensor(Λbar[6,:], (prev_V[5,:] > 0) * (1 - Λ[6, :])) #S\n",
    "    Λbar = T.inc_subtensor(Λbar[7,:], (prev_V[7,:] > 0) * (1 - Λ[7, :])) #G\n",
    "    Λbar = T.inc_subtensor(Λbar[8,:], (prev_V[7,:] > 0) * (1 - Λ[7, :])) #H\n",
    "    Λbar = T.inc_subtensor(Λbar[9,:], (prev_V[9,:] > 0) * (1 - Λ[9, :])) #Mcs\n",
    "    Λbar = T.inc_subtensor(Λbar[10,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #M1\n",
    "    Λbar = T.inc_subtensor(Λbar[11,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #N\n",
    "    Λbar = T.inc_subtensor(Λbar[12,:], (prev_V[11,:] > 0) * (1 - Λ[12, :])) #Ucs\n",
    "    Λbar = T.inc_subtensor(Λbar[13,:], (prev_V[9,:] > 0) * (1 - Λ[9, :])) #U1\n",
    "    Λbar = T.inc_subtensor(Λbar[14,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #U2\n",
    "    Λbar = T.inc_subtensor(Λbar[15,:], (prev_V[3,:] > 0) * (1 - Λ[3, :])) #D1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[16,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #D2_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[17,:], (prev_V[5,:] > 0) * (1 - Λ[5, :])) #E1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[18,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #M1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[19,:], (prev_V[9,:] > 0) * (1 - Λ[10, :])) #U1_abs\n",
    "    Λbar = T.inc_subtensor(Λbar[20,:], (prev_V[11,:] > 0) * (1 - Λ[11, :])) #U2_abs\n",
    "    \n",
    "   \n",
    "    #To make λbar = 1, activate \"λbar = T.ones_like(Λbar)\" code below and \n",
    "    #deactivate all \"λbar =\" code under \"Code for λbar Scaling\" below.\n",
    "    \n",
    "    #Code for λbar = 1\n",
    "#     λbar = T.ones_like(Λbar)\n",
    "    \n",
    "    #Code for λbar Scaling\n",
    "    λbar = T.zeros_like(Λbar)\n",
    "    λbar = T.inc_subtensor(λbar[0,:], prev_V[0,:]) #Dcs\n",
    "    λbar = T.inc_subtensor(λbar[1,:], prev_V[3,:]) #D1\n",
    "    λbar = T.inc_subtensor(λbar[2,:], prev_V[5,:]) #D2\n",
    "    λbar = T.inc_subtensor(λbar[3,:], prev_V[5,:]) #Ecs\n",
    "    λbar = T.inc_subtensor(λbar[4,:], prev_V[5,:]) #E1\n",
    "    λbar = T.inc_subtensor(λbar[5,:], prev_V[5,:]) #F\n",
    "    λbar = T.inc_subtensor(λbar[6,:], prev_V[5,:]) #S\n",
    "    λbar = T.inc_subtensor(λbar[7,:], prev_V[7,:]) #G\n",
    "    λbar = T.inc_subtensor(λbar[8,:], prev_V[7,:]) #H\n",
    "    λbar = T.inc_subtensor(λbar[9,:], prev_V[9,:]) #Mcs\n",
    "    λbar = T.inc_subtensor(λbar[10,:], prev_V[11,:]) #M1\n",
    "    λbar = T.inc_subtensor(λbar[11,:], prev_V[11,:]) #N\n",
    "    λbar = T.inc_subtensor(λbar[11,:], prev_V[12,:]) #Ucs\n",
    "    λbar = T.inc_subtensor(λbar[13,:], prev_V[9,:]) #U1\n",
    "    λbar = T.inc_subtensor(λbar[14,:], prev_V[11,:]) #U2\n",
    "    λbar = T.inc_subtensor(λbar[15,:], prev_V[3,:]) #D1_abs\n",
    "    λbar = T.inc_subtensor(λbar[16,:], prev_V[5,:]) #D2_abs\n",
    "    λbar = T.inc_subtensor(λbar[17,:], prev_V[5,:]) #E1_abs\n",
    "    λbar = T.inc_subtensor(λbar[18,:], prev_V[11,:]) #M1_abs\n",
    "    λbar = T.inc_subtensor(λbar[19,:], prev_V[9,:]) #U1_abs\n",
    "    λbar = T.inc_subtensor(λbar[20,:], prev_V[11,:]) #U2_abs\n",
    "\n",
    "    \n",
    "    #Prediction error\n",
    "    pe_V = λ - prev_V\n",
    "    pe_Vbar = λbar - prev_Vbar\n",
    "    pe_P = λ - prev_P\n",
    "    pe_N = λbar - prev_N\n",
    "    pe_P2 = λ - prev_P2\n",
    "    pe_N2 = λbar - prev_N2\n",
    "    \n",
    "    \n",
    "    #To make γ = 1, activate code under \"Code for γ = 1\" and \n",
    "    #deactivate code under \"Code for γ Scaling\" below.\n",
    "    \n",
    "    #Code for γ = 1\n",
    "#     prev_γ1 = T.zeros_like(prev_V)\n",
    "#     prev_γ1 = T.set_subtensor(prev_γ1[stimulus_type.nonzero(),:],1)\n",
    "#     prev_γ2 = T.zeros_like(prev_V)\n",
    "#     prev_γ2 = T.set_subtensor(prev_γ2[OS1_type.nonzero(),:],1)\n",
    "    \n",
    "    #Code for γ Scaling\n",
    "    prev_γ1 = prev_V * prev_Vbar\n",
    "    prev_γ2 = prev_P * prev_N\n",
    "\n",
    "    #Delta formulas\n",
    "    CS_prev_γ1 = (prev_γ1 * stimuli_shown).sum(axis=0)\n",
    "    CS_prev_γ2 = (prev_γ2 * stimuli_shown).sum(axis=0)\n",
    "    ΔV = Λ * (α) * pe_V\n",
    "    ΔVbar = Λbar * (α) * pe_Vbar\n",
    "    ΔP = Λ * (α) * CS_prev_γ1 * pe_P\n",
    "    ΔN = Λbar * (α) * CS_prev_γ1 * pe_N\n",
    "    ΔP2 = Λ * (α) * CS_prev_γ2 * pe_P2\n",
    "    ΔN2 = Λbar * (α) * CS_prev_γ2 * pe_N2\n",
    "\n",
    "\n",
    "    # Only update stimuli that were shown\n",
    "    ΔV = ΔV * stimuli_shown\n",
    "    ΔVbar = ΔVbar * stimuli_shown\n",
    "    ΔP = ΔP * stimuli_shown\n",
    "    ΔN = ΔN * stimuli_shown\n",
    "    ΔP2 = ΔP2 * stimuli_shown\n",
    "    ΔN2 = ΔN2 * stimuli_shown\n",
    "    \n",
    "    # Update V, Vbar, P, N, P2, N2\n",
    "    V = T.zeros_like(prev_V)\n",
    "    Vbar = T.zeros_like(prev_Vbar)\n",
    "    P = T.zeros_like(prev_P)\n",
    "    N = T.zeros_like(prev_N)\n",
    "    P2 = T.zeros_like(prev_P2)\n",
    "    N2 = T.zeros_like(prev_N2)\n",
    "    \n",
    "    # Only update V and Vbar for CSs. Only update P and N for 1st-order OSs. Only update P2 and N2 for 2nd-order OSs.\n",
    "    V = T.inc_subtensor(V[T.eq(stimulus_type, 1)], prev_V[T.eq(stimulus_type, 1)] + ΔV[T.eq(stimulus_type, 1)] * training_or_test)\n",
    "    Vbar = T.inc_subtensor(Vbar[T.eq(stimulus_type, 1)], prev_Vbar[T.eq(stimulus_type, 1)] + ΔVbar[T.eq(stimulus_type, 1)] * training_or_test)\n",
    "    P = T.inc_subtensor(P[T.eq(OS1_type, 1)], prev_P[T.eq(OS1_type, 1)] + ΔP[T.eq(OS1_type, 1)] * training_or_test)\n",
    "    N = T.inc_subtensor(N[T.eq(OS1_type, 1)], prev_N[T.eq(OS1_type, 1)] + ΔN[T.eq(OS1_type, 1)] * training_or_test)\n",
    "    P2 = T.inc_subtensor(P2[T.eq(OS2_type, 1)], prev_P2[T.eq(OS2_type, 1)] + ΔP2[T.eq(OS2_type, 1)] * training_or_test)\n",
    "    N2 = T.inc_subtensor(N2[T.eq(OS2_type, 1)], prev_N2[T.eq(OS2_type, 1)] + ΔN2[T.eq(OS2_type, 1)] * training_or_test)\n",
    "    \n",
    "    return V, Vbar, P, N, P2, N2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Simulated Data with Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In code below, use \"Code for randomized α values\" to simulate random α values, which will then be estimated by our model. This was used for our simulated vs recovered plots in the manuscript. In order to graph \"perfect\" learning rate data, use \"Code for α = 1,\" which was plotted in our manuscript alongside participants with low, medium, and high α parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = len(data['ID'].unique())\n",
    "n_stim = 21\n",
    "\n",
    "#Initial values\n",
    "R = np.zeros((n_stim, n_subjects))\n",
    "overall_R = np.zeros((1, n_subjects))\n",
    "v_excitatory = np.zeros((n_stim, n_subjects))\n",
    "v_inhibitory = np.zeros((n_stim, n_subjects))\n",
    "P = np.zeros((n_stim, n_subjects))\n",
    "N = np.zeros((n_stim, n_subjects))\n",
    "P2 = np.zeros((n_stim, n_subjects))\n",
    "N2 = np.zeros((n_stim, n_subjects))\n",
    "\n",
    "#Code for randomized α values\n",
    "gen_dist = pm.Beta.dist(2, 2, shape = n_subjects)\n",
    "α_subject_sim = gen_dist.random()\n",
    "\n",
    "\n",
    "#Code for α = 1\n",
    "# gen_dist = pm.Beta.dist(2, 2, shape=n_subjects)\n",
    "# α_subject_sim = np.ones(n_subjects)\n",
    "\n",
    "\n",
    "#Test vs Training Trial\n",
    "training_or_test = data.pivot(index='trialseq', values='Test', columns='ID').values[:, np.newaxis, :].astype(float)\n",
    "\n",
    "#US values\n",
    "small_lambda = data.pivot(index='trialseq', values='US', columns='ID').values[:, np.newaxis, :].repeat(n_stim, axis=1).astype(float)\n",
    "stim_data = []\n",
    "\n",
    "for sub in data['ID'].unique():\n",
    "    stim_data.append(data.loc[data['ID'] == sub, ['Dcs', 'D1', 'D2', 'Ecs', 'E1', 'F', 'S', 'G', 'H', 'Mcs', \n",
    "                                                  'M1', 'N', 'Ucs', 'U1', 'U2', 'D1_abs', 'D2_abs', 'E1_abs', \n",
    "                                                  'M1_abs', 'U1_abs', 'U2_abs']].values)\n",
    "\n",
    "stimuli_shown = np.dstack(stim_data)\n",
    "big_lambda = small_lambda\n",
    "\n",
    "#Add imaginary -1th trial\n",
    "big_lambda = np.vstack([np.zeros((1, n_stim, n_subjects)), big_lambda[:-1, ...]]).astype(float) # Add one trial of zeros to the start, remove the last trial\n",
    "small_lambda = big_lambda\n",
    "stimuli_shown = np.vstack([np.zeros((1, n_stim, n_subjects)), stimuli_shown]) # Add one trial of zeros to the start, DO NOT remove the last trial - this is needed for prediction\n",
    "\n",
    "stimulus_type = np.ones(n_stim)\n",
    "stimulus_type[[1, 4, 10, 13, 15, 17, 18, 19, 2, 14, 16, 20]] = 0 #make all OSs = 0\n",
    "\n",
    "OS1_type = np.zeros(n_stim)\n",
    "OS1_type[[1, 4, 10, 13, 15, 17, 18, 19]] = 1 #make 1st OSs = 1\n",
    "\n",
    "OS2_type = np.zeros(n_stim)\n",
    "OS2_type[[2, 14, 16, 20]] = 1 #make 2nd OSs = 1\n",
    "\n",
    "\n",
    "#Convert task outcomes to tensors\n",
    "big_lambda = T.as_tensor_variable(big_lambda.astype(float))\n",
    "small_lambda = T.as_tensor_variable(small_lambda.astype(float))\n",
    "stimuli_shown = T.as_tensor_variable(stimuli_shown)\n",
    "training_or_test = T.as_tensor_variable(training_or_test)\n",
    "\n",
    "stimuli_shown_sim = stimuli_shown.copy()\n",
    "big_lambda_sim = big_lambda.copy()\n",
    "small_lambda_sim = small_lambda.copy()\n",
    "training_or_test_sim = training_or_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Fake Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the loop\n",
    "output, updates = scan(fn=learning_function,\n",
    "                    sequences=[{'input': stimuli_shown_sim[:-1, ...]},\n",
    "                             {'input': big_lambda_sim},\n",
    "                             {'input': small_lambda_sim},\n",
    "                             {'input': training_or_test}],\n",
    "                    outputs_info=[v_excitatory, v_inhibitory, P, N, P2, N2],\n",
    "                    non_sequences = [stimulus_type, OS1_type, OS2_type, α_subject_sim])\n",
    "\n",
    "#Get model output\n",
    "V_out, Vbar_out, P_out, N_out, P2_out, N2_out = [i.eval() for i in output]\n",
    "\n",
    "estimated_overall_R = ((V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) - (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) + \\\n",
    "    ((P_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) - \\\n",
    "    ((N_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) + \\\n",
    "    ((P2_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (N_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (P_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (N_out * stimuli_shown_sim[1:, ...]).sum(axis=1)) - \\\n",
    "    ((N2_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (P_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (V_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (Vbar_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (P_out * stimuli_shown_sim[1:, ...]).sum(axis=1) * (N_out * stimuli_shown_sim[1:, ...]).sum(axis=1))\n",
    "\n",
    "overall_R_sim = estimated_overall_R.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check parameter recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = len(data['ID'].unique())\n",
    "\n",
    "#Initial values\n",
    "R = np.zeros((n_stim, n_subjects))\n",
    "\n",
    "#US values\n",
    "small_lambda = data.pivot(index='trialseq', values='US', columns='ID').values[:, np.newaxis, :].repeat(n_stim, axis=1).astype(float)\n",
    "stim_data = []\n",
    "\n",
    "for sub in data['ID'].unique():\n",
    "    stim_data.append(data.loc[data['ID'] == sub, ['Dcs', 'D1', 'D2', 'Ecs', 'E1', 'F', 'S', 'G', 'H', 'Mcs', \n",
    "                                                  'M1', 'N', 'Ucs', 'U1', 'U2', 'D1_abs', 'D2_abs', 'E1_abs', \n",
    "                                                  'M1_abs', 'U1_abs', 'U2_abs']].values)\n",
    "\n",
    "stimuli_shown = np.dstack(stim_data)\n",
    "big_lambda = small_lambda\n",
    "\n",
    "#Add imaginary -1th trial\n",
    "big_lambda = np.vstack([np.zeros((1, n_stim, n_subjects)), big_lambda[:-1, ...]]).astype(float) # Add one trial of zeros to the start, remove the last trial\n",
    "small_lambda = big_lambda\n",
    "stimuli_shown = np.vstack([np.zeros((1, n_stim, n_subjects)), stimuli_shown]) # Add one trial of zeros to the start, DO NOT remove the last trial - this is needed for prediction\n",
    "\n",
    "stimulus_type = np.ones(n_stim)\n",
    "stimulus_type[[1, 4, 10, 13, 15, 17, 18, 19, 2, 14, 16, 20]] = 0 #make all OSs = 0\n",
    "\n",
    "OS1_type = np.zeros(n_stim)\n",
    "OS1_type[[1, 4, 10, 13, 15, 17, 18, 19]] = 1 #make 1st OSs = 1\n",
    "\n",
    "OS2_type = np.zeros(n_stim)\n",
    "OS2_type[[2, 14, 16, 20]] = 1 #make 2nd OSs = 1\n",
    "\n",
    "#Convert task outcomes to tensors\n",
    "\n",
    "big_lambda = T.as_tensor_variable(big_lambda.astype(float))\n",
    "small_lambda = T.as_tensor_variable(small_lambda.astype(float))\n",
    "stimuli_shown = T.as_tensor_variable(stimuli_shown)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    \n",
    "    # Learning rate lies between 0 and 1 so we use a beta distribution\n",
    "    α_mean = pm.Normal('α_mean', 0.5, 10)\n",
    "    α_sd = pm.HalfCauchy('α_sd', 10)\n",
    "    \n",
    "\n",
    "    \n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=0, upper=1)\n",
    "    α_subject = BoundedNormal('α', mu=α_mean, sd=α_sd, shape=(n_subjects,))\n",
    "\n",
    "    \n",
    "    # Run the loop\n",
    "    output, updates = scan(fn=learning_function,\n",
    "                      sequences=[{'input': stimuli_shown[:-1, ...]},\n",
    "                             {'input': big_lambda},\n",
    "                             {'input': small_lambda},\n",
    "                             {'input': training_or_test}],\n",
    "                      outputs_info=[v_excitatory, v_inhibitory, P, N, P2, N2],\n",
    "                      non_sequences=[stimulus_type, OS1_type, OS2_type, α_subject])\n",
    "    \n",
    "    # Get model output\n",
    "    V, Vbar, P, N, P2, N2 = output\n",
    "    \n",
    "    # Calculate response - combine direct associations, 1st-order occasion setting, and 2nd-order occasion setting.\n",
    "    # As a reminder: γ1 = V*Vbar, and γ2 = P*N\n",
    "    R = (V - Vbar) + ((P * Vbar * V*Vbar) - (N * V *V*Vbar)) + (P2 * N * V * V*Vbar * P*N) - (N2 * P * Vbar * V*Vbar * P*N)\n",
    "\n",
    "    # # Single R value\n",
    "    estimated_overall_R = ((V * stimuli_shown[1:, ...]).sum(axis=1) - (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P2 * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N2 * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1))\n",
    "    \n",
    "    # This allows us to output the estimated R\n",
    "    estimated_overall_R = pm.Deterministic('estimated_overall_R', estimated_overall_R)\n",
    "    \n",
    "    # Reshape output of the model and get categorical likelihood\n",
    "    sigma = pm.HalfCauchy('sigma', 0.5)\n",
    "    likelihood = pm.Normal('likelihood', mu=estimated_overall_R, sigma=sigma, observed=pd.DataFrame(overall_R_sim.squeeze()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Data (go to \"Real Data\" below if you want to skip data simulations and just get real data results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.variational.callbacks import CheckParametersConvergence\n",
    "with model:\n",
    "    approx = pm.fit(method='advi', n=40000, callbacks=[CheckParametersConvergence()])\n",
    "trace = approx.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_output = pm.summary(trace, kind='stats', varnames=[i for i in model.named_vars if 'α' in i and not i in model.deterministics and not 'log' in i and not 'interval' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, var_names=['α_mean']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_data_var = {'Simulated_α': α_subject_sim, 'Recovered_α': trace['α'].mean(axis=0)}\n",
    "\n",
    "recovered_data_var = pd.DataFrame(recovered_data_var)\n",
    "recovered_data_var.to_csv(os.path.join('../output/',r'2nd POS - OS2 General, Simulated vs Recovered.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, sharex = True, sharey = True, figsize=(3, 2.5))\n",
    "f.suptitle('Simulated vs Recovered α Parameters', y=1.02, fontsize = 16)\n",
    "f.text(.5, -.02, 'Simulated α', va='center', ha='center', fontsize = 16)\n",
    "f.text(-.02, .5, 'Recovered α', va='center', ha='center', fontsize = 16, rotation=90)\n",
    "\n",
    "sns.regplot(α_subject_sim, trace['α'].mean(axis=0), label='α_subject', color = 'black')\n",
    "\n",
    "ax.set_title('α General')\n",
    "\n",
    "plt.setp(ax, xticks=[0, .2, .4, .6, .8, 1], yticks=[0, .2, .4, .6, .8, 1])        \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join('../output/',r'2nd POS - OS2 General, Simulated vs Recovered.svg'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model to Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = len(data['ID'].unique())\n",
    "\n",
    "# Initial values\n",
    "R = np.zeros((n_stim, n_subjects))  # Value estimate\n",
    "overall_R = np.zeros((1, n_subjects))\n",
    "v_excitatory = np.zeros((n_stim, n_subjects)) \n",
    "v_inhibitory = np.zeros((n_stim, n_subjects)) \n",
    "P = np.zeros((n_stim, n_subjects))\n",
    "N = np.zeros((n_stim, n_subjects))\n",
    "P2 = np.zeros((n_stim, n_subjects))\n",
    "N2 = np.zeros((n_stim, n_subjects))\n",
    "gamma = np.ones((n_stim, n_subjects))\n",
    "\n",
    "# US values\n",
    "small_lambda = data.pivot(index='trialseq', values='US', columns='ID').values[:, np.newaxis, :].repeat(n_stim, axis=1)\n",
    "stim_data = []\n",
    "\n",
    "for sub in data['ID'].unique():\n",
    "    stim_data.append(data.loc[data['ID'] == sub, ['Dcs', 'D1', 'D2', 'Ecs', 'E1', 'F', 'S', 'G', 'H', 'Mcs', \n",
    "                                                  'M1', 'N', 'Ucs', 'U1', 'U2', 'D1_abs', 'D2_abs', 'E1_abs', \n",
    "                                                  'M1_abs', 'U1_abs', 'U2_abs']].values)\n",
    "    \n",
    "stimuli_shown = np.dstack(stim_data)\n",
    "big_lambda = small_lambda\n",
    "\n",
    "# Add imaginary -1th trial\n",
    "big_lambda = np.vstack([np.zeros((1, n_stim, n_subjects)), big_lambda[:-1, ...]])  # Add one trial of zeros to the start, remove the last trial\n",
    "small_lambda = big_lambda\n",
    "stimuli_shown = np.vstack([np.zeros((1, n_stim, n_subjects)), stimuli_shown]) # Add one trial of zeros to the start, DO NOT remove the last trial - this is needed for prediction\n",
    "\n",
    "stimulus_type = np.ones(n_stim)\n",
    "stimulus_type[[1, 4, 10, 13, 15, 17, 18, 19, 2, 14, 16, 20]] = 0 #make all OSs = 0\n",
    "\n",
    "OS1_type = np.zeros(n_stim)\n",
    "OS1_type[[1, 4, 10, 13, 15, 17, 18, 19]] = 1 #make 1st OSs = 1\n",
    "\n",
    "OS2_type = np.zeros(n_stim)\n",
    "OS2_type[[2, 14, 16, 20]] = 1 #make 2nd OSs = 1\n",
    "\n",
    "# Convert task outcomes to tensors\n",
    "big_lambda = T.as_tensor_variable(big_lambda)\n",
    "small_lambda = T.as_tensor_variable(small_lambda)\n",
    "stimuli_shown = T.as_tensor_variable(stimuli_shown)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    \n",
    "    # Learning rate lies between 0 and 1 so we use a beta distribution\n",
    "    α_mean = pm.Normal('α_mean', 0.5, 10)\n",
    "    α_sd = pm.HalfCauchy('α_sd', 10)\n",
    "\n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=0, upper=1)\n",
    "    α_subject = BoundedNormal('α', mu=α_mean, sd=α_sd, shape=(n_subjects,))\n",
    "\n",
    "    \n",
    "    # Run the loop\n",
    "    output, updates = scan(fn=learning_function,\n",
    "                      sequences=[dict(input=stimuli_shown[:-1, ...]), dict(input=big_lambda), dict(input=small_lambda), dict(input=training_or_test)],\n",
    "                      outputs_info=[v_excitatory, v_inhibitory, P, N, P2, N2],\n",
    "                      non_sequences=[stimulus_type, OS1_type, OS2_type, α_subject])\n",
    "\n",
    "    # Get model output\n",
    "    V, Vbar, P, N, P2, N2 = output\n",
    "\n",
    "    \n",
    "    # Calculate response - combine direct associations, 1st-order occasion setting, and 2nd-order occasion setting.\n",
    "    # As a reminder: γ1 = V*Vbar, and γ2 = P*N\n",
    "    R = (V - Vbar) + ((P * Vbar * V*Vbar) - (N * V *V*Vbar)) + (P2 * N * V * V*Vbar * P*N) - (N2 * P * Vbar * V*Vbar * P*N)\n",
    "\n",
    "    # # Single R value\n",
    "    estimated_overall_R = ((V * stimuli_shown[1:, ...]).sum(axis=1) - (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1)) + \\\n",
    "        ((P2 * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1)) - \\\n",
    "        ((N2 * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (V * stimuli_shown[1:, ...]).sum(axis=1) * (Vbar * stimuli_shown[1:, ...]).sum(axis=1) * (P * stimuli_shown[1:, ...]).sum(axis=1) * (N * stimuli_shown[1:, ...]).sum(axis=1))\n",
    "       \n",
    "    # This allows us to output the estimated R\n",
    "    estimated_overall_R = pm.Deterministic('estimated_overall_R', estimated_overall_R)\n",
    "          \n",
    "    # Reshape output of the model and get categorical likelihood\n",
    "    sigma = pm.HalfCauchy('sigma', 0.5)\n",
    "    likelihood = pm.Normal('likelihood', mu=estimated_overall_R, sigma=sigma, observed=pd.DataFrame(observed_R.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.variational.callbacks import CheckParametersConvergence\n",
    "with model:\n",
    "    approx = pm.fit(method='advi', n=40000, callbacks=[CheckParametersConvergence()])\n",
    "trace = approx.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_output = pm.summary(trace, kind='stats', varnames=[i for i in model.named_vars if 'α' in i and not i in model.deterministics and not 'log' in i and not 'interval' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, var_names=['α_mean']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAIC, R2, and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_R_mean = trace['estimated_overall_R'].mean(axis=0)\n",
    "overall_R_sd = trace['estimated_overall_R'].std(axis=0)\n",
    "\n",
    "sub_ids = data['ID'].unique()\n",
    "\n",
    "subs = [np.where(data['ID'].unique() == sub)[0][0] for sub in sub_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2s = []\n",
    "\n",
    "for n, sub in enumerate(data['ID'].unique()):\n",
    "    r2s.append(r2_score(observed_R.squeeze()[~np.isnan(observed_R.squeeze()[:, n]), n], overall_R_mean[~np.isnan(observed_R.squeeze()[:, n]), n]))\n",
    "\n",
    "group_r2 = np.median(r2s)\n",
    "\n",
    "print(group_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2s = {'R2': [r2s]}\n",
    "\n",
    "r2s = pd.DataFrame(r2s)\n",
    "\n",
    "r2s.to_csv(os.path.join('../output/',r'2nd POS - OS2 General, R2 Output.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_output = pm.waic(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_output.to_csv(os.path.join('../output/',r'2nd POS - OS2 General, Alpha Output.csv'))\n",
    "waic_output.to_csv(os.path.join('../output/',r'2nd POS - OS2 General, WAIC Output.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Real, Model-Predicted, and Perfect Learning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('../data/', \"2nd POS Modeling All Data OS2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DV'] = ((data['DV'].values - 1) / 2) - 1\n",
    "\n",
    "observed_R_test = data.pivot(columns = 'ID', index = 'trialseq', values = 'DV').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the following cell, you will need to have run the \"Simulation Data\" code near the top of this notebook using \"Code for α = 1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(23, 3, figsize=(36, 48), dpi = 100)\n",
    "\n",
    "overall_R_mean = trace['estimated_overall_R'].mean(axis=0)\n",
    "overall_R_sd = trace['estimated_overall_R'].std(axis=0)\n",
    "\n",
    "sub_ids = data['ID'].unique()\n",
    "\n",
    "subs = [np.where(data['ID'].unique() == sub)[0][0] for sub in sub_ids]\n",
    "    \n",
    "for n, sub in enumerate(subs):\n",
    "    ax[n % 23, int(n / 23)].fill_between(range(overall_R_mean.shape[0]), overall_R_mean[:, sub] - overall_R_sd[:, sub], overall_R_mean[:, sub] + overall_R_sd[:, sub], alpha=0.3)\n",
    "    ax[n % 23, int(n / 23)].plot(overall_R_mean[:, sub])\n",
    "    ax[n % 23, int(n / 23)].plot(observed_R_test.squeeze()[:, sub], color='orange', linestyle='-')#participant's real data\n",
    "    ax[n % 23, int(n / 23)].plot(overall_R_sim.squeeze()[:, sub], color='silver', linestyle=':', alpha = .7)#Alpha = 1; this is the correct answer if a person learned perfectly\n",
    "    if n == 0:\n",
    "        ax[n % 23, int(n / 23)].set_ylabel('Mean (+/-SD) overall R')\n",
    "    ax[n % 23, int(n / 23)].set_ylabel('Responding (R)')\n",
    "    ax[n % 23, int(n / 23)].set_xlabel('Trials')\n",
    "    ax[n % 23, int(n / 23)].set_title('Sub {0}'.format(sub_ids[n]))    \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join('../output/',r'2nd POS - OS2 General, Individual Real and Estimated Responding.svg'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p conda,jupyterlab,numpy,pandas,theano,pymc3,sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
